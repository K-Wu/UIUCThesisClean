% Generated by IEEEtran.bst, version: 1.12 (2007/01/11)
\begin{thebibliography}{100}
\providecommand{\url}[1]{#1}
\csname url@samestyle\endcsname
\providecommand{\newblock}{\relax}
\providecommand{\bibinfo}[2]{#2}
\providecommand{\BIBentrySTDinterwordspacing}{\spaceskip=0pt\relax}
\providecommand{\BIBentryALTinterwordstretchfactor}{4}
\providecommand{\BIBentryALTinterwordspacing}{\spaceskip=\fontdimen2\font plus
\BIBentryALTinterwordstretchfactor\fontdimen3\font minus \fontdimen4\font\relax}
\providecommand{\BIBforeignlanguage}[2]{{%
\expandafter\ifx\csname l@#1\endcsname\relax
\typeout{** WARNING: IEEEtran.bst: No hyphenation pattern has been}%
\typeout{** loaded for the language `#1'. Using the pattern for}%
\typeout{** the default language instead.}%
\else
\language=\csname l@#1\endcsname
\fi
#2}}
\providecommand{\BIBdecl}{\relax}
\BIBdecl

\bibitem{Pinterest}
\BIBentryALTinterwordspacing
R.~Ying, R.~He, K.~Chen, P.~Eksombatchai, W.~L. Hamilton, and J.~Leskovec, ``Graph convolutional neural networks for web-scale recommender systems,'' in \emph{Proceedings of the 24th ACM SIGKDD International Conference on Knowledge Discovery \& Data Mining}, ser. KDD '18.\hskip 1em plus 0.5em minus 0.4em\relax New York, NY, USA: Association for Computing Machinery, 2018. [Online]. Available: \url{https://doi.org/10.1145/3219819.3219890} p. 974–983.
\BIBentrySTDinterwordspacing

\bibitem{naumovDeepLearningRecommendation2019}
\BIBentryALTinterwordspacing
M.~Naumov, D.~Mudigere, H.-J.~M. Shi, J.~Huang, N.~Sundaraman, J.~Park, X.~Wang, U.~Gupta, C.-J. Wu, A.~G. Azzolini, D.~Dzhulgakov, A.~Mallevich, I.~Cherniavskii, Y.~Lu, R.~Krishnamoorthi, A.~Yu, V.~Kondratenko, S.~Pereira, X.~Chen, W.~Chen, V.~Rao, B.~Jia, L.~Xiong, and M.~Smelyanskiy, ``Deep learning recommendation model for personalization and recommendation systems,'' 2019. [Online]. Available: \url{http://arxiv.org/abs/1906.00091}
\BIBentrySTDinterwordspacing

\bibitem{openaiChatGPT2022}
\BIBentryALTinterwordspacing
{OpenAI}, ``{{ChatGPT}},'' 2022. [Online]. Available: \url{https://chatgpt.com/}
\BIBentrySTDinterwordspacing

\bibitem{midjourneyMidjourney2022}
\BIBentryALTinterwordspacing
{Midjourney}, ``Midjourney,'' 2022. [Online]. Available: \url{https://www.midjourney.com}
\BIBentrySTDinterwordspacing

\bibitem{villalobosTradingComputeTraining2023}
\BIBentryALTinterwordspacing
P.~Villalobos, ``Trading off compute in training and inference,'' 2023, Epoch AI. [Online]. Available: \url{https://epochai.org/blog/trading-off-compute-in-training-and-inference}
\BIBentrySTDinterwordspacing

\bibitem{billdallyDeepLearningHardware2022}
\BIBentryALTinterwordspacing
B.~Dally, ``Deep learning hardware: Past, present, and future,'' 2022. [Online]. Available: \url{https://oc.acm.org/docs/DL_HW_OC_ACM_0322.pdf}
\BIBentrySTDinterwordspacing

\bibitem{jouppiTPUV4}
\BIBentryALTinterwordspacing
N.~Jouppi, G.~Kurian, S.~Li, P.~Ma, R.~Nagarajan, L.~Nai, N.~Patil, S.~Subramanian, A.~Swing, B.~Towles, C.~Young, X.~Zhou, Z.~Zhou, and D.~A. Patterson, ``{{TPU}} v4: An optically reconfigurable supercomputer for machine learning with hardware support for embeddings,'' in \emph{Proceedings of the 50th {{Annual International Symposium}} on {{Computer Architecture}}}.\hskip 1em plus 0.5em minus 0.4em\relax {ACM}, 2023. [Online]. Available: \url{https://dl.acm.org/doi/10.1145/3579371.3589350} pp. 1--14.
\BIBentrySTDinterwordspacing

\bibitem{liLLMAnalysisLatencyMemory2023}
\BIBentryALTinterwordspacing
C.~Li, ``{{LLM-Analysis}}: Latency and memory analysis of transformer models for training and inference,'' 2023, accessed 07/04/2024. [Online]. Available: \url{https://github.com/cli99/llm-analysis}
\BIBentrySTDinterwordspacing

\bibitem{yuanLLMInferenceUnveiled2024}
\BIBentryALTinterwordspacing
Z.~Yuan, Y.~Shang, Y.~Zhou, Z.~Dong, Z.~Zhou, C.~Xue, B.~Wu, Z.~Li, Q.~Gu, Y.~J. Lee, Y.~Yan, B.~Chen, G.~Sun, and K.~Keutzer, ``{{LLM}} inference unveiled: Survey and roofline model insights,'' 2024. [Online]. Available: \url{http://arxiv.org/abs/2402.16363}
\BIBentrySTDinterwordspacing

\bibitem{epochParameterComputeData}
\BIBentryALTinterwordspacing
{{Epoch AI}}, ``Parameter, compute and data trends in machine learning,'' 2022. [Online]. Available: \url{https://epochai.org/mlinputs/visualization}
\BIBentrySTDinterwordspacing

\bibitem{techpowerupGPUSpecsDatabase2024}
\BIBentryALTinterwordspacing
{TechPowerUp}, ``{{GPU}} specs database,'' Feb. 2024. [Online]. Available: \url{https://www.techpowerup.com/gpu-specs/}
\BIBentrySTDinterwordspacing

\bibitem{timothyprickettmorganLotsQuestionsGoogle2024}
\BIBentryALTinterwordspacing
T.~P. Morgan, ``Lots of questions on {{Google}}'s `{{Trillium}}' {{TPU}} v6, a few answers,'' 2024. [Online]. Available: \url{https://www.nextplatform.com/2024/06/10/lots-of-questions-on-googles-trillium-tpu-v6-a-few-answers/}
\BIBentrySTDinterwordspacing

\bibitem{smithNVIDIABlackwellArchitecture2024}
\BIBentryALTinterwordspacing
R.~Smith, ``{{NVIDIA Blackwell}} architecture and {{B200}}/{{B100}} accelerators announced: Going bigger with smaller data,'' 2024. [Online]. Available: \url{https://www.anandtech.com/show/21310/nvidia-blackwell-architecture-and-b200b100-accelerators-announced-going-bigger-with-smaller-data}
\BIBentrySTDinterwordspacing

\bibitem{TensorProcessingUnit2017}
\BIBentryALTinterwordspacing
Wikipedia, ``Tensor processing unit,'' 2017. [Online]. Available: \url{https://en.wikipedia.org/w/index.php?title=Tensor_Processing_Unit}
\BIBentrySTDinterwordspacing

\bibitem{jouppiInDatacenterPerformanceAnalysis2017}
N.~P. Jouppi, C.~Young, N.~Patil, D.~Patterson, G.~Agrawal, R.~Bajwa, S.~Bates, S.~Bhatia, N.~Boden, A.~Borchers, R.~Boyle, P.-l. Cantin, C.~Chao, C.~Clark, J.~Coriell, M.~Daley, M.~Dau, J.~Dean, B.~Gelb, T.~V. Ghaemmaghami, R.~Gottipati, W.~Gulland, R.~Hagmann, C.~R. Ho, D.~Hogberg, J.~Hu, R.~Hundt, D.~Hurt, J.~Ibarz, A.~Jaffey, A.~Kaplan, H.~Khaitan, D.~Killebrew, A.~Koch, N.~Kumar, S.~Lacy, J.~Laudon, J.~Law, D.~Le, C.~Leary, Z.~Liu, K.~Lucke, A.~Lundin, G.~MacKean, A.~Maggiore, M.~Mahony, K.~Miller, R.~Nagarajan, R.~Narayanaswami, N.~Penukonda, A.~Phelps, J.~Ross, M.~Ross, A.~Salek, E.~Samadiani, C.~Severn, G.~Sizikov, M.~Snelham, J.~Souter, D.~Steinberg, A.~Swing, M.~Tan, G.~Thorson, B.~Tian, H.~Toma, E.~Tuttle, V.~Vasudevan, R.~Walter, W.~Wang, E.~Wilcox, and D.~H. Yoon, ``In-datacenter performance analysis of a tensor processing unit,'' in \emph{Proceedings of the 44th {{International Symposium}} on {{Computer Architecture}}}, 2017.

\bibitem{minLargeGraphConvolutional2021}
\BIBentryALTinterwordspacing
S.~W. Min, K.~Wu, S.~Huang, M.~Hidayeto{\u g}lu, J.~Xiong, E.~Ebrahimi, D.~Chen, and W.-M. Hwu, ``Large graph convolutional network training with {{GPU-oriented}} data communication architecture,'' \emph{Proceedings of the VLDB Endowment}, vol.~14, no.~11, pp. 2087--2100, July 2021. [Online]. Available: \url{https://dl.acm.org/doi/10.14778/3476249.3476264}
\BIBentrySTDinterwordspacing

\bibitem{minEMOGIEfficientMemoryaccess2020}
\BIBentryALTinterwordspacing
S.~W. Min, V.~S. Mailthody, Z.~Qureshi, J.~Xiong, E.~Ebrahimi, and W.-M. Hwu, ``{{EMOGI}}: Efficient memory-access for out-of-memory graph-traversal in {{GPUs}},'' \emph{{Proceedings of the VLDB Endowment}}, vol.~14, no.~2, pp. 114--127, 2020. [Online]. Available: \url{https://dl.acm.org/doi/10.14778/3425879.3425883}
\BIBentrySTDinterwordspacing

\bibitem{minFinegrainedMemoryAccess2022}
\BIBentryALTinterwordspacing
S.~W. Min, ``Fine-grained memory access over {{I}}/{{O}} interconnect for efficient remote sparse data access,'' Thesis, University of Illinois at Urbana-Champaign, 2022. [Online]. Available: \url{https://hdl.handle.net/2142/115489}
\BIBentrySTDinterwordspacing

\bibitem{techpowerupEnterpriseSSDDatabase2024}
\BIBentryALTinterwordspacing
{TechPowerUp}, ``Enterprise {{SSD}} database.'' [Online]. Available: \url{https://www.techpowerup.com/ssd-specs/search/?market=2}
\BIBentrySTDinterwordspacing

\bibitem{derekjonesShapeCodeMemory2020}
\BIBentryALTinterwordspacing
D.~Jones, ``Memory capacity growth: A major contributor to the success of computers,'' 2020. [Online]. Available: \url{https://shape-of-code.com/2020/10/04/memory-capacity-growth-a-major-contributor-to-the-success-of-computers/}
\BIBentrySTDinterwordspacing

\bibitem{brooksNoSilverBullet1987}
\BIBentryALTinterwordspacing
F.~P. {Brooks Jr.}, ``No silver bullet essence and accidents of software engineering,'' \emph{Computer}, vol.~20, no.~4, pp. 10--19, 1987. [Online]. Available: \url{https://ieeexplore.ieee.org/document/1663532}
\BIBentrySTDinterwordspacing

\bibitem{CuBLAS}
\BIBentryALTinterwordspacing
Nvidia, ``{{cuBLAS}} library user guide v12.0,'' Dec. 2022. [Online]. Available: \url{https://docs.nvidia.com/cuda/cublas/index.html}
\BIBentrySTDinterwordspacing

\bibitem{CUTLASS2022}
\BIBentryALTinterwordspacing
Nvidia, ``{{CUTLASS}},'' NVIDIA Corporation, Dec. 2022. [Online]. Available: \url{https://github.com/NVIDIA/cutlass}
\BIBentrySTDinterwordspacing

\bibitem{wuHectorEfficientProgramming2024}
\BIBentryALTinterwordspacing
K.~Wu, M.~Hidayetoğlu, X.~Song, S.~Huang, D.~Zheng, I.~Nisa, and W.-M. Hwu, ``Hector: An efficient programming and compilation framework for implementing relational graph neural networks in {{GPU}} architectures,'' in \emph{Proceedings of the 29th {{ACM International Conference}} on {{Architectural Support}} for {{Programming Languages}} and {{Operating Systems}}, {{Volume}} 3}, ser. {{ASPLOS}} '24, vol.~3.\hskip 1em plus 0.5em minus 0.4em\relax Association for Computing Machinery, 2024. [Online]. Available: \url{https://dl.acm.org/doi/10.1145/3620666.3651322} pp. 528--544.
\BIBentrySTDinterwordspacing

\bibitem{min2021pytorchdirect}
\BIBentryALTinterwordspacing
S.~W. Min, K.~Wu, S.~Huang, M.~Hidayetoğlu, J.~Xiong, E.~Ebrahimi, D.~Chen, and W.-M. Hwu, ``{{PyTorch-Direct}}: Enabling {{GPU}} centric data access for very large graph neural network training with irregular accesses,'' 2021. [Online]. Available: \url{https://arxiv.org/abs/2101.07956}
\BIBentrySTDinterwordspacing

\bibitem{minGraphNeuralNetwork2022}
\BIBentryALTinterwordspacing
S.~W. Min, K.~Wu, M.~Hidayetoğlu, J.~Xiong, X.~Song, and W.-M. Hwu, ``Graph neural network training with data tiering,'' in \emph{Proceedings of the 28th {{ACM SIGKDD Conference}} on {{Knowledge Discovery}} and {{Data Mining}}}.\hskip 1em plus 0.5em minus 0.4em\relax {Association for Computing Machinery}, 2022. [Online]. Available: \url{https://dl.acm.org/doi/abs/10.1145/3534678.3539038} pp. 3555--3565.
\BIBentrySTDinterwordspacing

\bibitem{wuTBAFasterLarge2024}
\BIBentryALTinterwordspacing
K.~Wu, J.~B. Park, X.~Zhang, M.~Hidayetoğlu, V.~S. Mailthody, S.~Huang, S.~S. Lumetta, and W.-M. Hwu, ``{{TBA}}: Faster large language model training using {{SSD}}-based activation offloading,'' 2024. [Online]. Available: \url{http://arxiv.org/abs/2408.10013}
\BIBentrySTDinterwordspacing

\bibitem{CNN0}
Y.~{LeCun}, B.~{Boser}, J.~S. {Denker}, D.~{Henderson}, R.~E. {Howard}, W.~{Hubbard}, and L.~D. {Jackel}, ``Backpropagation applied to handwritten zip code recognition,'' \emph{Neural Computation}, vol.~1, no.~4, pp. 541--551, 1989.

\bibitem{lecunGCN}
J.~Bruna, W.~Zaremba, A.~Szlam, and Y.~Lecun, ``\BIBforeignlanguage{English (US)}{Spectral networks and locally connected networks on graphs},'' in \emph{\BIBforeignlanguage{English (US)}{International Conference on Learning Representations (ICLR2014), CBLS, April 2014}}, 2014.

\bibitem{hamilton2017inductive}
W.~L. Hamilton, R.~Ying, and J.~Leskovec, ``Inductive representation learning on large graphs,'' in \emph{Proceedings of the 31st International Conference on Neural Information Processing Systems}, ser. NIPS'17.\hskip 1em plus 0.5em minus 0.4em\relax Red Hook, NY, USA: Curran Associates Inc., 2017, p. 1025–1035.

\bibitem{GCNPierre}
M.~Defferrard, X.~Bresson, and P.~Vandergheynst, ``Convolutional neural networks on graphs with fast localized spectral filtering,'' in \emph{Proceedings of the 30th International Conference on Neural Information Processing Systems}, ser. NIPS'16.\hskip 1em plus 0.5em minus 0.4em\relax Red Hook, NY, USA: Curran Associates Inc., 2016, p. 3844–3852.

\bibitem{kipf2016semi}
\BIBentryALTinterwordspacing
T.~N. Kipf and M.~Welling, ``Semi-supervised classification with graph convolutional networks,'' \emph{arXiv preprint arXiv:1609.02907}, 2016. [Online]. Available: \url{https://arxiv.org/abs/1609.02907}
\BIBentrySTDinterwordspacing

\bibitem{kipf2016variational}
T.~N. Kipf and M.~Welling, ``Variational graph auto-encoders,'' \emph{NIPS Workshop on Bayesian Deep Learning}, 2016.

\bibitem{pmlr-v48-niepert16}
\BIBentryALTinterwordspacing
M.~Niepert, M.~Ahmed, and K.~Kutzkov, ``Learning convolutional neural networks for graphs,'' in \emph{Proceedings of The 33rd International Conference on Machine Learning}, ser. Proceedings of Machine Learning Research, M.~F. Balcan and K.~Q. Weinberger, Eds., vol.~48.\hskip 1em plus 0.5em minus 0.4em\relax New York, New York, USA: PMLR, 20--22 Jun 2016. [Online]. Available: \url{http://proceedings.mlr.press/v48/niepert16.html} pp. 2014--2023.
\BIBentrySTDinterwordspacing

\bibitem{HamiltonYL17}
\BIBentryALTinterwordspacing
W.~L. Hamilton, R.~Ying, and J.~Leskovec, ``Representation learning on graphs: Methods and applications,'' \emph{{IEEE} Data Eng. Bull.}, vol.~40, no.~3, pp. 52--74, 2017. [Online]. Available: \url{http://sites.computer.org/debull/A17sept/p52.pdf}
\BIBentrySTDinterwordspacing

\bibitem{DeepWalk}
\BIBentryALTinterwordspacing
B.~Perozzi, R.~Al-Rfou, and S.~Skiena, ``{{DeepWalk}}: Online learning of social representations,'' in \emph{Proceedings of the 20th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining}, ser. KDD '14.\hskip 1em plus 0.5em minus 0.4em\relax New York, NY, USA: Association for Computing Machinery, 2014. [Online]. Available: \url{https://doi.org/10.1145/2623330.2623732} p. 701–710.
\BIBentrySTDinterwordspacing

\bibitem{node2vec}
\BIBentryALTinterwordspacing
A.~Grover and J.~Leskovec, ``Node2vec: Scalable feature learning for networks,'' in \emph{Proceedings of the 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining}, ser. KDD '16.\hskip 1em plus 0.5em minus 0.4em\relax New York, NY, USA: Association for Computing Machinery, 2016. [Online]. Available: \url{https://doi.org/10.1145/2939672.2939754} p. 855–864.
\BIBentrySTDinterwordspacing

\bibitem{BingChatMicrosoft2023}
\BIBentryALTinterwordspacing
Microsoft, ``Bing {{Chat}} | {{Microsoft Edge}},'' 2023. [Online]. Available: \url{https://www.microsoft.com/en-us/edge/features/bing-chat}
\BIBentrySTDinterwordspacing

\bibitem{langchainLangChain2022}
\BIBentryALTinterwordspacing
{LangChain}, ``{{LangChain}},'' 2022. [Online]. Available: \url{https://github.com/langchain-ai/langchain}
\BIBentrySTDinterwordspacing

\bibitem{weiEmergentAbilitiesLarge2022}
\BIBentryALTinterwordspacing
J.~Wei, Y.~Tay, R.~Bommasani, C.~Raffel, B.~Zoph, S.~Borgeaud, D.~Yogatama, M.~Bosma, D.~Zhou, D.~Metzler, E.~H. Chi, T.~Hashimoto, O.~Vinyals, P.~Liang, J.~Dean, and W.~Fedus, ``Emergent abilities of large language models,'' 2022. [Online]. Available: \url{http://arxiv.org/abs/2206.07682}
\BIBentrySTDinterwordspacing

\bibitem{radfordLanguageModelsAre2019}
\BIBentryALTinterwordspacing
A.~Radford, J.~Wu, R.~Child, D.~Luan, D.~Amodei, and I.~Sutskever, ``Language models are unsupervised multitask learners,'' 2019. [Online]. Available: \url{https://d4mucfpksywv.cloudfront.net/better-language-models/language_models_are_unsupervised_multitask_learners.pdf}
\BIBentrySTDinterwordspacing

\bibitem{vaswaniAttentionAllYou2017}
A.~Vaswani, N.~Shazeer, N.~Parmar, J.~Uszkoreit, L.~Jones, A.~N. Gomez, {\L}.~Kaiser, and I.~Polosukhin, ``Attention is all you need,'' in \emph{Advances in {{Neural Information Processing Systems}}}, vol.~30.\hskip 1em plus 0.5em minus 0.4em\relax Curran Associates, Inc., 2017.

\bibitem{devlinBERTPretrainingDeep2019}
\BIBentryALTinterwordspacing
J.~Devlin, M.-W. Chang, K.~Lee, and K.~Toutanova, ``{{BERT}}: Pre-training of deep bidirectional transformers for language understanding,'' \emph{arXiv preprint}, no. 1810.04805, May 2019. [Online]. Available: \url{https://arxiv.org/abs/1810.04805}
\BIBentrySTDinterwordspacing

\bibitem{touvronLlamaOpenFoundation2023}
\BIBentryALTinterwordspacing
H.~Touvron, L.~Martin, K.~Stone, P.~Albert, A.~Almahairi, Y.~Babaei, N.~Bashlykov, S.~Batra, P.~Bhargava, S.~Bhosale, D.~Bikel, L.~Blecher, C.~C. Ferrer, M.~Chen, G.~Cucurull, D.~Esiobu, J.~Fernandes, J.~Fu, W.~Fu, B.~Fuller, C.~Gao, V.~Goswami, N.~Goyal, A.~Hartshorn, S.~Hosseini, R.~Hou, H.~Inan, M.~Kardas, V.~Kerkez, M.~Khabsa, I.~Kloumann, A.~Korenev, P.~S. Koura, M.-A. Lachaux, T.~Lavril, J.~Lee, D.~Liskovich, Y.~Lu, Y.~Mao, X.~Martinet, T.~Mihaylov, P.~Mishra, I.~Molybog, Y.~Nie, A.~Poulton, J.~Reizenstein, R.~Rungta, K.~Saladi, A.~Schelten, R.~Silva, E.~M. Smith, R.~Subramanian, X.~E. Tan, B.~Tang, R.~Taylor, A.~Williams, J.~X. Kuan, P.~Xu, Z.~Yan, I.~Zarov, Y.~Zhang, A.~Fan, M.~Kambadur, S.~Narang, A.~Rodriguez, R.~Stojnic, S.~Edunov, and T.~Scialom, ``Llama 2: Open foundation and fine-tuned chat models,'' \emph{arXiv preprint}, no. 2307.09288, July 2023. [Online]. Available: \url{https://arxiv.org/abs/2307.09288}
\BIBentrySTDinterwordspacing

\bibitem{raffelExploringLimitsTransfer2023}
\BIBentryALTinterwordspacing
C.~Raffel, N.~Shazeer, A.~Roberts, K.~Lee, S.~Narang, M.~Matena, Y.~Zhou, W.~Li, and P.~J. Liu, ``Exploring the limits of transfer learning with a unified text-to-text transformer,'' \emph{arXiv preprint}, no. 1910.10683, Sep. 2023. [Online]. Available: \url{https://arxiv.org/abs/1910.10683}
\BIBentrySTDinterwordspacing

\bibitem{xuGSPMDGeneralScalable2021}
\BIBentryALTinterwordspacing
Y.~Xu, H.~Lee, D.~Chen, B.~Hechtman, Y.~Huang, R.~Joshi, M.~Krikun, D.~Lepikhin, A.~Ly, M.~Maggioni, R.~Pang, N.~Shazeer, S.~Wang, T.~Wang, Y.~Wu, and Z.~Chen, ``{{GSPMD}}: General and scalable parallelization for {{ML}} computation graphs,'' \emph{arXiv preprint}, no. 2105.04663, Dec. 2021. [Online]. Available: \url{https://arxiv.org/abs/2105.04663}
\BIBentrySTDinterwordspacing

\bibitem{shoeybiMegatronLMTrainingMultiBillion2020a}
\BIBentryALTinterwordspacing
M.~Shoeybi, M.~Patwary, R.~Puri, P.~LeGresley, J.~Casper, and B.~Catanzaro, ``Megatron-{{LM}}: Training multi-billion parameter language models using model parallelism,'' \emph{arXiv preprint}, no. 1909.08053, Mar. 2020. [Online]. Available: \url{https://arxiv.org/abs/1909.08053}
\BIBentrySTDinterwordspacing

\bibitem{rasleyDeepSpeedSystemOptimizations2020}
J.~Rasley, S.~Rajbhandari, O.~Ruwase, and Y.~He, ``{{DeepSpeed}}: System optimizations enable training deep learning models with over 100 billion parameters,'' in \emph{Proceedings of the 26th {{ACM SIGKDD International Conference}} on {{Knowledge Discovery}} \& {{Data Mining}}}.\hskip 1em plus 0.5em minus 0.4em\relax Virtual Event CA USA: ACM, Aug. 2020, pp. 3505--3506.

\bibitem{anselPyTorchFasterMachine2024}
\BIBentryALTinterwordspacing
J.~Ansel, E.~Yang, H.~He, N.~Gimelshein, A.~Jain, M.~Voznesensky, B.~Bao, P.~Bell, D.~Berard, E.~Burovski, G.~Chauhan, A.~Chourdia, W.~Constable, A.~Desmaison, Z.~DeVito, E.~Ellison, W.~Feng, J.~Gong, M.~Gschwind, B.~Hirsh, S.~Huang, K.~Kalambarkar, L.~Kirsch, M.~Lazos, M.~Lezcano, Y.~Liang, J.~Liang, Y.~Lu, C.~K. Luk, B.~Maher, Y.~Pan, C.~Puhrsch, M.~Reso, M.~Saroufim, M.~Y. Siraichi, H.~Suk, S.~Zhang, M.~Suo, P.~Tillet, X.~Zhao, E.~Wang, K.~Zhou, R.~Zou, X.~Wang, A.~Mathews, W.~Wen, G.~Chanan, P.~Wu, and S.~Chintala, ``{{PyTorch}} 2: Faster machine learning through dynamic python bytecode transformation and graph compilation,'' in \emph{Proceedings of the 29th {{ACM International Conference}} on {{Architectural Support}} for {{Programming Languages}} and {{Operating Systems}}, {{Volume}} 2}.\hskip 1em plus 0.5em minus 0.4em\relax ACM, 2024. [Online]. Available: \url{https://dl.acm.org/doi/10.1145/3620665.3640366} pp. 929--947.
\BIBentrySTDinterwordspacing

\bibitem{rajbhandariZeROMemoryOptimizations2020a}
S.~Rajbhandari, J.~Rasley, O.~Ruwase, and Y.~He, ``{{ZeRO}}: Memory optimizations toward training trillion parameter models,'' in \emph{{{SC20}}: {{International Conference}} for {{High Performance Computing}}, {{Networking}}, {{Storage}} and {{Analysis}}}.\hskip 1em plus 0.5em minus 0.4em\relax Atlanta, GA, USA: IEEE, Nov. 2020, pp. 1--16.

\bibitem{PMPP4}
\BIBentryALTinterwordspacing
W.-M.~W. Hwu, D.~B. Kirk, and I.~{El Hajj}, \emph{Programming Massively Parallel Processors}, 4th~ed.\hskip 1em plus 0.5em minus 0.4em\relax Morgan Kaufmann, 2023. [Online]. Available: \url{https://www.sciencedirect.com/book/9780323912310/programming-massively-parallel-processors}
\BIBentrySTDinterwordspacing

\bibitem{V100Whitepaper}
\BIBentryALTinterwordspacing
Nvidia, ``Nvidia {{Tesla}} {{V100}} {{GPU}} architecture whitepaper,'' 2017. [Online]. Available: \url{https://images.nvidia.com/content/volta-architecture/pdf/volta-architecture-whitepaper.pdf}
\BIBentrySTDinterwordspacing

\bibitem{jiaDissectingNVIDIAVolta2018}
\BIBentryALTinterwordspacing
Z.~Jia, M.~Maggioni, B.~Staiger, and D.~P. Scarpazza, ``Dissecting the {{NVIDIA Volta GPU}} architecture via microbenchmarking.'' [Online]. Available: \url{http://arxiv.org/abs/1804.06826}
\BIBentrySTDinterwordspacing

\bibitem{nickollsInstructionsManagingParallel2019}
\BIBentryALTinterwordspacing
J.~R. Nickolls, B.~W. Coon, and M.~C. Shebanow, ``Instructions for managing a parallel cache hierarchy,'' U.S. Patent 10\,365\,930B2, 2019. [Online]. Available: \url{https://patents.google.com/patent/US10365930/en}
\BIBentrySTDinterwordspacing

\bibitem{davidm.koppelmanEE7722GPU2023}
\BIBentryALTinterwordspacing
D.~M. Koppelman, ``{{EE}} 7722 {{GPU}} microarchitecture lecture notes,'' 2023. [Online]. Available: \url{https://www.ece.lsu.edu/koppel/gp/notes/set-nv-org.pdf}
\BIBentrySTDinterwordspacing

\bibitem{nvidiaKernelProfilingGuide2024}
\BIBentryALTinterwordspacing
{Nvidia}, ``2. kernel profiling guide — {{NsightCompute}} 12.6 documentation,'' 2024. [Online]. Available: \url{https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#id27}
\BIBentrySTDinterwordspacing

\bibitem{geeksforgeeksSortingVectorTuple2020}
\BIBentryALTinterwordspacing
{GeeksforGeeks}, ``Sorting of vector of tuple in {{C}}++ (ascending order),'' 2020. [Online]. Available: \url{https://www.geeksforgeeks.org/sorting-vector-tuple-c-ascending-order/}
\BIBentrySTDinterwordspacing

\bibitem{wikipediaCPython2008}
\BIBentryALTinterwordspacing
{Wikipedia}, ``{{CPython}},'' 2008. [Online]. Available: \url{https://en.wikipedia.org/w/index.php?title=CPython}
\BIBentrySTDinterwordspacing

\bibitem{CanPytorchBypass2019}
\BIBentryALTinterwordspacing
``Can pytorch by-pass {{Python}} {{GIL}}?'' 2019, PyTorch Forums. [Online]. Available: \url{https://discuss.pytorch.org/t/can-pytorch-by-pass-python-gil/55498}
\BIBentrySTDinterwordspacing

\bibitem{wikipediaIronPython2006}
\BIBentryALTinterwordspacing
{Wikipedia}, ``{{IronPython}},'' 2006. [Online]. Available: \url{https://en.wikipedia.org/wiki/IronPython}
\BIBentrySTDinterwordspacing

\bibitem{samgrossPEP703Making2023}
\BIBentryALTinterwordspacing
S.~Gross, ``{{PEP}} 703 – making the global interpreter lock optional in {{CPython}} | peps.python.org,'' Python Enhancement Proposals (PEPs). [Online]. Available: \url{https://peps.python.org/pep-0703/}
\BIBentrySTDinterwordspacing

\bibitem{joergAutomatedGPUKernel2019}
\BIBentryALTinterwordspacing
T.~Joerg, ``Automated {{GPU}} kernel fusion with {{XLA}}.'' [Online]. Available: \url{https://llvm.org/devmtg/2019-04/slides/TechTalk-Joerg-Automated_GPU_Kernel_Fusion_with_XLA.pdf}
\BIBentrySTDinterwordspacing

\bibitem{pengwuWorkshopsASPLOS_2024README}
\BIBentryALTinterwordspacing
P.~Wu, J.~Ansel, H.~He, A.~Jain, M.~Lezcano, M.~Lazos, P.~Bell, A.~Chaudhuri, B.~Bao, B.~Hirsh, E.~Ellison, Y.~Chen, and B.~Feng, ``{{PyTorch}} 2 tutorial and paper presentation @ {{ASPLOS}}'2024.'' [Online]. Available: \url{https://github.com/pytorch/workshops/blob/master/ASPLOS_2024/README.md}
\BIBentrySTDinterwordspacing

\bibitem{dakkakCompilingHighlevelScripting2020}
\BIBentryALTinterwordspacing
A.~M. Dakkak, ``Compiling high-level scripting languages to performant code,'' Dissertation, University of Illinois at Urbana-Champaign, 2020. [Online]. Available: \url{https://hdl.handle.net/2142/108715}
\BIBentrySTDinterwordspacing

\bibitem{zygoteFluxMLZygoteJl2019}
\BIBentryALTinterwordspacing
{Zygote}, ``Zygote: 21st century {{AD}}.'' [Online]. Available: \url{https://github.com/FluxML/Zygote.jl}
\BIBentrySTDinterwordspacing

\bibitem{ioanaifrimGPUAccelerationAutomatic2021}
\BIBentryALTinterwordspacing
I.~Ifrim, ``{{GPU}} acceleration of automatic differentiation in {{C}}++ with {{Clad}},'' 2021. [Online]. Available: \url{https://indico.cern.ch/event/1040761/contributions/4400258/attachments/2268253/3851595/Ioana%20Ifrim%20-%20GPU%20Acceleration%20of%20Automatic%20Differentiation%20in%20C%2B%2B%20with%20Clad.pdf}
\BIBentrySTDinterwordspacing

\bibitem{mosesReversemodeAutomaticDifferentiation2021}
\BIBentryALTinterwordspacing
W.~S. Moses, V.~Churavy, L.~Paehler, J.~Hückelheim, S.~H.~K. Narayanan, M.~Schanen, and J.~Doerfert, ``Reverse-mode automatic differentiation and optimization of {{GPU}} kernels via {{Enzyme}},'' in \emph{Proceedings of the {{International Conference}} for {{High Performance Computing}}, {{Networking}}, {{Storage}} and {{Analysis}}}, vol.~12.\hskip 1em plus 0.5em minus 0.4em\relax ACM, 2021. [Online]. Available: \url{https://dl.acm.org/doi/10.1145/3458817.3476165} pp. 1--16.
\BIBentrySTDinterwordspacing

\bibitem{wenzeljakobPybindPybind112016}
\BIBentryALTinterwordspacing
W.~Jakob, ``pybind11 — seamless operability between c++11 and python,'' 2016. [Online]. Available: \url{https://github.com/pybind/pybind11}
\BIBentrySTDinterwordspacing

\bibitem{wang2019deep}
\BIBentryALTinterwordspacing
M.~Wang, D.~Zheng, Z.~Ye, Q.~Gan, M.~Li, X.~Song, J.~Zhou, C.~Ma, L.~Yu, Y.~Gai et~al., ``Deep graph library: A graph-centric, highly-performant package for graph neural networks,'' \emph{arXiv preprint arXiv:1909.01315}, 2019. [Online]. Available: \url{https://arxiv.org/abs/1909.01315}
\BIBentrySTDinterwordspacing

\bibitem{fey2019fast}
\BIBentryALTinterwordspacing
M.~Fey and J.~E. Lenssen, ``Fast graph representation learning with {{PyTorch Geometric}},'' \emph{arXiv preprint arXiv:1903.02428}, 2019. [Online]. Available: \url{https://arxiv.org/abs/1903.02428}
\BIBentrySTDinterwordspacing

\bibitem{huFeatGraphFlexibleEfficient2020a}
\BIBentryALTinterwordspacing
Y.~Hu, Z.~Ye, M.~Wang, J.~Yu, D.~Zheng, M.~Li, Z.~Zhang, Z.~Zhang, and Y.~Wang, ``{{FeatGraph}}: A flexible and efficient backend for graph neural network systems,'' in \emph{{{SC20}}: {{International Conference}} for {{High Performance Computing}}, {{Networking}}, {{Storage}} and {{Analysis}}}.\hskip 1em plus 0.5em minus 0.4em\relax {Atlanta, GA, USA}: {IEEE}, Nov. 2020. [Online]. Available: \url{https://ieeexplore.ieee.org/document/9355318/} pp. 1--13.
\BIBentrySTDinterwordspacing

\bibitem{huangEfficientSparseMatrix2021}
\BIBentryALTinterwordspacing
G.~Huang, G.~Dai, Y.~Wang, Y.~Ding, and Y.~Xie, ``Efficient sparse matrix kernels based on adaptive workload-balancing and parallel-reduction,'' Oct. 2021. [Online]. Available: \url{http://arxiv.org/abs/2106.16064}
\BIBentrySTDinterwordspacing

\bibitem{yeSparseTIRComposableAbstractions2022}
\BIBentryALTinterwordspacing
Z.~Ye, R.~Lai, J.~Shao, T.~Chen, and L.~Ceze, ``{{SparseTIR}}: Composable abstractions for sparse compilation in deep learning,'' in \emph{Proceedings of the 28th ACM International Conference on Architectural Support for Programming Languages and Operating Systems, Volume 3}, ser. ASPLOS 2023.\hskip 1em plus 0.5em minus 0.4em\relax New York, NY, USA: Association for Computing Machinery, 2023. [Online]. Available: \url{https://doi.org/10.1145/3582016.3582047} p. 660–678.
\BIBentrySTDinterwordspacing

\bibitem{rgcn}
\BIBentryALTinterwordspacing
M.~Schlichtkrull, T.~N. Kipf, P.~Bloem, R.~{van~den Berg}, I.~Titov, and M.~Welling, ``Modeling relational data with graph convolutional networks,'' in \emph{The Semantic Web}, A.~Gangemi, R.~Navigli, M.-E. Vidal, P.~Hitzler, R.~Troncy, L.~Hollink, A.~Tordai, and M.~Alam, Eds.\hskip 1em plus 0.5em minus 0.4em\relax {Cham}: {Springer International Publishing}, 2018, vol. 10843, pp. 593--607. [Online]. Available: \url{http://link.springer.com/10.1007/978-3-319-93417-4_38}
\BIBentrySTDinterwordspacing

\bibitem{hgt}
\BIBentryALTinterwordspacing
Z.~Hu, Y.~Dong, K.~Wang, and Y.~Sun, ``Heterogeneous graph transformer,'' in \emph{Proceedings of {{The Web Conference}} 2020}.\hskip 1em plus 0.5em minus 0.4em\relax {Taipei Taiwan}: {ACM}, Apr. 2020. [Online]. Available: \url{https://dl.acm.org/doi/10.1145/3366423.3380027} pp. 2704--2710.
\BIBentrySTDinterwordspacing

\bibitem{wangEmpiricalAnalysisPerformance2021}
\BIBentryALTinterwordspacing
Z.~Wang, Y.~Wang, C.~Yuan, R.~Gu, and Y.~Huang, ``Empirical analysis of performance bottlenecks in graph neural network training and inference with {{GPUs}},'' \emph{Neurocomputing}, vol. 446, pp. 165--191, July 2021. [Online]. Available: \url{https://www.sciencedirect.com/science/article/pii/S0925231221003659}
\BIBentrySTDinterwordspacing

\bibitem{zhengNatureGraphNeural2021}
\BIBentryALTinterwordspacing
D.~Zheng and G.~Karypis, ``The nature of graph neural network workloads,'' Aug. 2021. [Online]. Available: \url{https://hc33.hotchips.org/assets/program/tutorials/HC2021.Amazon.DaZheng.v2.pdf}
\BIBentrySTDinterwordspacing

\bibitem{nvidiaCublasgemmBatchedCuBLASDocuemntation}
\BIBentryALTinterwordspacing
Nvidia, ``{{cublas}}{$<$}t{$>$}{{gemmBatched}}() | {{cuBLAS}} library user guide v12.2,'' July 2023. [Online]. Available: \url{https://docs.nvidia.com/cuda/cublas/index.html#cublas-t-gemmbatched#:~:text=make%20multiple%20calls%20to%20cublas%3Ct%3Egemm}
\BIBentrySTDinterwordspacing

\bibitem{AcceleratingMatrixMultiplication}
\BIBentryALTinterwordspacing
Nvidia, ``Accelerating matrix multiplication with block sparse format and {{NVIDIA}} tensor cores | {{NVIDIA}} technical blog,'' Mar. 2021. [Online]. Available: \url{https://developer.nvidia.com/blog/accelerating-matrix-multiplication-with-block-sparse-format-and-nvidia-tensor-cores/}
\BIBentrySTDinterwordspacing

\bibitem{nisaFeatureGatherMm}
\BIBentryALTinterwordspacing
I.~Nisa, ``[feature] gather mm by isratnisa {$\cdot$} pull request \#3641 {$\cdot$} dmlc/dgl,'' Jan. 2022. [Online]. Available: \url{https://github.com/dmlc/dgl/pull/3641}
\BIBentrySTDinterwordspacing

\bibitem{wuSeastarVertexcentricProgramming2021}
\BIBentryALTinterwordspacing
Y.~Wu, K.~Ma, Z.~Cai, T.~Jin, B.~Li, C.~Zheng, J.~Cheng, and F.~Yu, ``Seastar: Vertex-centric programming for graph neural networks,'' in \emph{Proceedings of the {{Sixteenth European Conference}} on {{Computer Systems}}}.\hskip 1em plus 0.5em minus 0.4em\relax {Online Event United Kingdom}: {ACM}, Apr. 2021. [Online]. Available: \url{https://dl.acm.org/doi/10.1145/3447786.3456247} pp. 359--375.
\BIBentrySTDinterwordspacing

\bibitem{xieGraphilerCompilerGraph}
\BIBentryALTinterwordspacing
Z.~Xie, M.~Wang, Z.~Ye, Z.~Zhang, and R.~Fan, ``Graphiler: Optimizing graph neural networks with message passing data flow graph,'' in \emph{Proceedings of Machine Learning and Systems}, D.~Marculescu, Y.~Chi, and C.~Wu, Eds., vol.~4, 2022. [Online]. Available: \url{https://proceedings.mlsys.org/paper/2022/file/a87ff679a2f3e71d9181a67b7542122c-Paper.pdf} pp. 515--528.
\BIBentrySTDinterwordspacing

\bibitem{guiHGLAcceleratingHeterogeneous}
\BIBentryALTinterwordspacing
Y.~Gui, Y.~Wu, H.~Yang, T.~Jin, B.~Li, Q.~Zhou, J.~Cheng, and F.~Yu, ``{{HGL}}: Accelerating heterogeneous {{GNN}} training with holistic representation and optimization,'' in \emph{Proceedings of the {{International Conference}} on {{High Performance Computing}}, {{Networking}}, {{Storage}} and {{Analysis}}}, 2022. [Online]. Available: \url{https://dl.acm.org/doi/abs/10.5555/3571885.3571980} pp. 1--15.
\BIBentrySTDinterwordspacing

\bibitem{busbridge2019relational}
\BIBentryALTinterwordspacing
D.~Busbridge, D.~Sherburn, P.~Cavallo, and N.~Y. Hammerla, ``Relational graph attention networks,'' \emph{arXiv preprint arXiv:1904.05811}, 2019. [Online]. Available: \url{https://arxiv.org/abs/1904.05811}
\BIBentrySTDinterwordspacing

\bibitem{huOpenGraphBenchmark2021}
\BIBentryALTinterwordspacing
W.~Hu, M.~Fey, M.~Zitnik, Y.~Dong, H.~Ren, B.~Liu, M.~Catasta, and J.~Leskovec, ``Open graph benchmark: Datasets for machine learning on graphs,'' Feb. 2021. [Online]. Available: \url{http://arxiv.org/abs/2005.00687}
\BIBentrySTDinterwordspacing

\bibitem{aifb}
\BIBentryALTinterwordspacing
S.~Bloehdorn and Y.~Sure, ``Kernel methods for mining instance data in ontologies,'' in \emph{The {{Semantic Web}}}, D.~Hutchison, T.~Kanade, J.~Kittler, J.~M. Kleinberg, F.~Mattern, J.~C. Mitchell, M.~Naor, O.~Nierstrasz, C.~Pandu~Rangan, B.~Steffen, M.~Sudan, D.~Terzopoulos, D.~Tygar, M.~Y. Vardi, G.~Weikum, K.~Aberer, K.-S. Choi, N.~Noy, D.~Allemang, K.-I. Lee, L.~Nixon, J.~Golbeck, P.~Mika, D.~Maynard, R.~Mizoguchi, G.~Schreiber, and P.~{Cudr{\'e}-Mauroux}, Eds.\hskip 1em plus 0.5em minus 0.4em\relax {Berlin, Heidelberg}: {Springer Berlin Heidelberg}, 2007, vol. 4825, pp. 58--71. [Online]. Available: \url{http://link.springer.com/10.1007/978-3-540-76298-0_5}
\BIBentrySTDinterwordspacing

\bibitem{mutag}
\BIBentryALTinterwordspacing
A.~K. Debnath, R.~L. {Lopez de Compadre}, G.~Debnath, A.~J. Shusterman, and C.~Hansch, ``Structure-activity relationship of mutagenic aromatic and heteroaromatic nitro compounds. correlation with molecular orbital energies and hydrophobicity,'' \emph{Journal of Medicinal Chemistry}, vol.~34, no.~2, pp. 786--797, Feb. 1991. [Online]. Available: \url{https://pubs.acs.org/doi/abs/10.1021/jm00106a046}
\BIBentrySTDinterwordspacing

\bibitem{bgs}
\BIBentryALTinterwordspacing
G.~K.~D. {de Vries}, ``A fast approximation of the weisfeiler-lehman graph kernel for {{RDF}} data,'' in \emph{Advanced {{Information Systems Engineering}}}, D.~Hutchison, T.~Kanade, J.~Kittler, J.~M. Kleinberg, F.~Mattern, J.~C. Mitchell, M.~Naor, O.~Nierstrasz, C.~Pandu~Rangan, B.~Steffen, M.~Sudan, D.~Terzopoulos, D.~Tygar, M.~Y. Vardi, G.~Weikum, C.~Salinesi, M.~C. Norrie, and {\'O}.~Pastor, Eds.\hskip 1em plus 0.5em minus 0.4em\relax {Berlin, Heidelberg}: {Springer Berlin Heidelberg}, 2013, vol. 7908, pp. 606--621. [Online]. Available: \url{http://link.springer.com/10.1007/978-3-642-40988-2_39}
\BIBentrySTDinterwordspacing

\bibitem{am}
\BIBentryALTinterwordspacing
V.~{de Boer}, J.~Wielemaker, J.~{van Gent}, M.~Hildebrand, A.~Isaac, J.~{van Ossenbruggen}, and G.~Schreiber, ``Supporting linked data production for cultural heritage institutes: The {{Amsterdam}} museum case study,'' in \emph{The {{Semantic Web}}: {{Research}} and {{Applications}}}, D.~Hutchison, T.~Kanade, J.~Kittler, J.~M. Kleinberg, F.~Mattern, J.~C. Mitchell, M.~Naor, O.~Nierstrasz, C.~Pandu~Rangan, B.~Steffen, M.~Sudan, D.~Terzopoulos, D.~Tygar, M.~Y. Vardi, G.~Weikum, E.~Simperl, P.~Cimiano, A.~Polleres, O.~Corcho, and V.~Presutti, Eds.\hskip 1em plus 0.5em minus 0.4em\relax {Berlin, Heidelberg}: {Springer Berlin Heidelberg}, 2012, vol. 7295, pp. 733--747. [Online]. Available: \url{http://link.springer.com/10.1007/978-3-642-30284-8_56}
\BIBentrySTDinterwordspacing

\bibitem{toutanovaObservedLatentFeatures2015}
\BIBentryALTinterwordspacing
K.~Toutanova and D.~Chen, ``Observed versus latent features for knowledge base and text inference,'' in \emph{Proceedings of the 3rd {{Workshop}} on {{Continuous Vector Space Models}} and Their {{Compositionality}}}.\hskip 1em plus 0.5em minus 0.4em\relax {Beijing, China}: {Association for Computational Linguistics}, July 2015. [Online]. Available: \url{https://aclanthology.org/W15-4007} pp. 57--66.
\BIBentrySTDinterwordspacing

\bibitem{lavinFastAlgorithmsConvolutional2015}
A.~Lavin and S.~Gray, ``Fast algorithms for convolutional neural networks,'' in \emph{2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)}, 2016, pp. 4013--4021.

\bibitem{isratsegmm}
I.~Nisa, M.~Wang, D.~Zheng, Q.~Fu, {\"U}.~\c{C}ataly{\"u}rek, and G.~Karypis, ``Optimizing irregular dense operators of heterogeneous gnn models on gpu,'' in \emph{2023 IEEE International Parallel and Distributed Processing Symposium Workshops (IPDPSW)}, 2023, pp. 199--206.

\bibitem{pytorchTorchScriptPyTorchDocumentation}
\BIBentryALTinterwordspacing
{PyTorch}, ``{{TorchScript}} --- {{PyTorch}} 2.2 documentation,'' Jan. 2024. [Online]. Available: \url{https://pytorch.org/docs/stable/jit.html}
\BIBentrySTDinterwordspacing

\bibitem{lamNumbaLLVMbasedPython2015}
\BIBentryALTinterwordspacing
S.~K. Lam, A.~Pitrou, and S.~Seibert, ``Numba: A {{LLVM}}-based {{Python}} {{JIT}} compiler,'' in \emph{Proceedings of the {{Second Workshop}} on the {{LLVM Compiler Infrastructure}} in {{HPC}}}, ser. {{LLVM}} '15.\hskip 1em plus 0.5em minus 0.4em\relax New York, NY, USA: Association for Computing Machinery, Nov. 2015. [Online]. Available: \url{https://dl.acm.org/doi/10.1145/2833157.2833162} pp. 1--6.
\BIBentrySTDinterwordspacing

\bibitem{xieGraphilerRepositoryGithub2023}
\BIBentryALTinterwordspacing
Z.~Xie and Z.~Ye, ``Graphiler,'' Jan. 2023. [Online]. Available: \url{https://github.com/xiezhq-hermann/graphiler}
\BIBentrySTDinterwordspacing

\bibitem{chenTVMAutomatedEndtoEnd2018}
\BIBentryALTinterwordspacing
T.~Chen, T.~Moreau, Z.~Jiang, L.~Zheng, E.~Yan, H.~Shen, M.~Cowan, L.~Wang, Y.~Hu, L.~Ceze, C.~Guestrin, and A.~Krishnamurthy, ``{{TVM}}: An automated end-to-end optimizing compiler for deep learning,'' in \emph{13th {{USENIX Symposium}} on {{Operating Systems Design}} and {{Implementation}} ({{OSDI}} 18)}, 2018. [Online]. Available: \url{https://www.usenix.org/conference/osdi18/presentation/chen} pp. 578--594.
\BIBentrySTDinterwordspacing

\bibitem{huangGESpMMGeneralPurposeSparse2020}
\BIBentryALTinterwordspacing
G.~Huang, G.~Dai, Y.~Wang, and H.~Yang, ``{{GE-SpMM}}: General-purpose sparse matrix-matrix multiplication on {{GPUs}} for graph neural networks,'' in \emph{{{SC20}}: {{International Conference}} for {{High Performance Computing}}, {{Networking}}, {{Storage}} and {{Analysis}}}, Nov. 2020. [Online]. Available: \url{https://dl.acm.org/doi/10.5555/3433701.3433796} pp. 1--12.
\BIBentrySTDinterwordspacing

\bibitem{hidayetogluAtScaleSparseDeep2020}
\BIBentryALTinterwordspacing
M.~Hidayetoğlu, C.~Pearson, V.~S. Mailthody, E.~Ebrahimi, J.~Xiong, R.~Nagi, and W.-M. Hwu, ``At-scale sparse deep neural network inference with efficient gpu implementation,'' in \emph{2020 IEEE High Performance Extreme Computing Conference (HPEC)}, 2020. [Online]. Available: \url{https://doi.org/10.1109/HPEC43674.2020.9286206} pp. 1--7.
\BIBentrySTDinterwordspacing

\bibitem{fuTLPGNNLightweightTwoLevel2022}
\BIBentryALTinterwordspacing
Q.~Fu, Y.~Ji, and H.~H. Huang, ``{{TLPGNN}}: A lightweight two-level parallelism paradigm for graph neural network computation on {{GPU}},'' in \emph{Proceedings of the 31st {{International Symposium}} on {{High-Performance Parallel}} and {{Distributed Computing}}}, ser. {{HPDC}} '22.\hskip 1em plus 0.5em minus 0.4em\relax {New York, NY, USA}: {Association for Computing Machinery}, June 2022. [Online]. Available: \url{https://doi.org/10.1145/3502181.3531467} pp. 122--134.
\BIBentrySTDinterwordspacing

\bibitem{kjolstadTensorAlgebraCompiler2017}
\BIBentryALTinterwordspacing
F.~Kjolstad, S.~Kamil, S.~Chou, D.~Lugato, and S.~Amarasinghe, ``The tensor algebra compiler,'' \emph{Proceedings of the ACM on Programming Languages}, vol.~1, no. OOPSLA, pp. 1--29, Oct. 2017. [Online]. Available: \url{https://dl.acm.org/doi/10.1145/3133901}
\BIBentrySTDinterwordspacing

\bibitem{lattnerMLIRScalingCompiler2021}
\BIBentryALTinterwordspacing
C.~Lattner, M.~Amini, U.~Bondhugula, A.~Cohen, A.~Davis, J.~A. Pienaar, R.~Riddle, T.~Shpeisman, N.~Vasilache, and O.~Zinenko, ``{{MLIR}}: Scaling compiler infrastructure for domain specific computation,'' in \emph{{{CGO}} 2021}, 2021. [Online]. Available: \url{https://ieeexplore.ieee.org/document/9370308}
\BIBentrySTDinterwordspacing

\bibitem{rahmanFusedMMUnifiedSDDMMSpMM2021}
\BIBentryALTinterwordspacing
M.~K. Rahman, M.~H. Sujon, and A.~Azad, ``{{FusedMM}}: A unified sddmm-spmm kernel for graph embedding and graph neural networks,'' in \emph{2021 {{IEEE International Parallel}} and {{Distributed Processing Symposium}} ({{IPDPS}})}, May 2021. [Online]. Available: \url{https://ieeexplore.ieee.org/document/9460486} pp. 256--266.
\BIBentrySTDinterwordspacing

\bibitem{yesilDenseDynamicBlocks2022}
\BIBentryALTinterwordspacing
S.~Yesil, J.~E. Moreira, and J.~Torrellas, ``Dense dynamic blocks: Optimizing {{SpMM}} for processors with vector and matrix units using machine learning techniques,'' in \emph{Proceedings of the 36th {{ACM International Conference}} on {{Supercomputing}}}.\hskip 1em plus 0.5em minus 0.4em\relax {ACM}, 2022. [Online]. Available: \url{https://dl.acm.org/doi/10.1145/3524059.3532369} pp. 1--14.
\BIBentrySTDinterwordspacing

\bibitem{hegdeExTensorAcceleratorSparse2019}
\BIBentryALTinterwordspacing
K.~Hegde, H.~Asghari-Moghaddam, M.~Pellauer, N.~Crago, A.~Jaleel, E.~Solomonik, J.~Emer, and C.~W. Fletcher, ``{{ExTensor}}: An accelerator for sparse tensor algebra,'' in \emph{Proceedings of the 52nd {{Annual IEEE}}/{{ACM International Symposium}} on {{Microarchitecture}}}.\hskip 1em plus 0.5em minus 0.4em\relax {ACM}, 2019. [Online]. Available: \url{https://dl.acm.org/doi/10.1145/3352460.3358275} pp. 319--333.
\BIBentrySTDinterwordspacing

\bibitem{vuducAutomaticPerformanceTuning2003}
\BIBentryALTinterwordspacing
R.~Vuduc, ``Automatic performance tuning of sparse matrix kernels,'' Ph.D. dissertation, {University of California, Berkeley}, 2003. [Online]. Available: \url{https://bebop.cs.berkeley.edu/pubs/vuduc2003-dissertation.pdf}
\BIBentrySTDinterwordspacing

\bibitem{yangDesignPrinciplesSparse2018}
\BIBentryALTinterwordspacing
C.~Yang, A.~Buluç, and J.~D. Owens, ``Design principles for sparse matrix multiplication on the {{GPU}},'' in \emph{Euro-{{Par}} 2018: {{Parallel Processing}}}, M.~Aldinucci, L.~Padovani, and M.~Torquati, Eds.\hskip 1em plus 0.5em minus 0.4em\relax {Springer International Publishing}, 2018, vol. 11014, pp. 672--687. [Online]. Available: \url{https://link.springer.com/10.1007/978-3-319-96983-1_48}
\BIBentrySTDinterwordspacing

\bibitem{liuCSR5EfficientStorage2015}
\BIBentryALTinterwordspacing
W.~Liu and B.~Vinter, ``{{CSR5}}: An efficient storage format for cross-platform sparse matrix-vector multiplication,'' in \emph{Proceedings of the 29th {{ACM}} on {{International Conference}} on {{Supercomputing}}}.\hskip 1em plus 0.5em minus 0.4em\relax {ACM}, 2015. [Online]. Available: \url{https://dl.acm.org/doi/10.1145/2751205.2751209} pp. 339--350.
\BIBentrySTDinterwordspacing

\bibitem{yanYaSpMVAnotherSpMV2014}
\BIBentryALTinterwordspacing
S.~Yan, C.~Li, Y.~Zhang, and H.~Zhou, ``{{yaSpMV}}: Yet another {{SpMV}} framework on {{GPUs}},'' in \emph{Proceedings of the 19th {{ACM SIGPLAN}} Symposium on {{Principles}} and Practice of Parallel Programming}.\hskip 1em plus 0.5em minus 0.4em\relax {ACM}, 2014. [Online]. Available: \url{https://dl.acm.org/doi/10.1145/2555243.2555255} pp. 107--118.
\BIBentrySTDinterwordspacing

\bibitem{STRZODKA2012429}
\BIBentryALTinterwordspacing
R.~Strzodka, ``Chapter 31 - abstraction for {{AoS}} and {{SoA}} layout in {{C}}++,'' in \emph{GPU Computing Gems Jade Edition}, ser. Applications of GPU Computing Series, W.-M.~W. Hwu, Ed.\hskip 1em plus 0.5em minus 0.4em\relax Boston: Morgan Kaufmann, 2012, pp. 429--441. [Online]. Available: \url{https://www.sciencedirect.com/science/article/pii/B9780123859631000319}
\BIBentrySTDinterwordspacing

\bibitem{HOMANN2018325}
\BIBentryALTinterwordspacing
H.~Homann and F.~Laenen, ``{{SoAx}}: A generic {{C++}} structure of arrays for handling particles in {{HPC}} codes,'' \emph{Computer Physics Communications}, vol. 224, pp. 325--332, 2018. [Online]. Available: \url{https://www.sciencedirect.com/science/article/pii/S0010465517303983}
\BIBentrySTDinterwordspacing

\bibitem{zhengDistDGLDistributedGraph2020}
\BIBentryALTinterwordspacing
D.~Zheng, C.~Ma, M.~Wang, J.~Zhou, Q.~Su, X.~Song, Q.~Gan, Z.~Zhang, and G.~Karypis, ``{{DistDGL}}: Distributed graph neural network training for billion-scale graphs,'' in \emph{2020 {{IEEE}}/{{ACM}} 10th {{Workshop}} on {{Irregular Applications}}: {{Architectures}} and {{Algorithms}} ({{IA3}})}.\hskip 1em plus 0.5em minus 0.4em\relax {GA, USA}: {IEEE}, Nov. 2020. [Online]. Available: \url{https://doi.org/10.1109/IA351965.2020.00011} pp. 36--44.
\BIBentrySTDinterwordspacing

\bibitem{kjolstadTensorAlgebraCompilation2019}
\BIBentryALTinterwordspacing
F.~Kjolstad, W.~Ahrens, S.~Kamil, and S.~Amarasinghe, ``Tensor algebra compilation with workspaces,'' in \emph{2019 {{IEEE}}/{{ACM International Symposium}} on {{Code Generation}} and {{Optimization}} ({{CGO}})}.\hskip 1em plus 0.5em minus 0.4em\relax {Washington, DC, USA}: {IEEE}, Feb. 2019. [Online]. Available: \url{https://doi.org/10.1109/CGO.2019.8661185} pp. 180--192.
\BIBentrySTDinterwordspacing

\bibitem{Chiang2019ClusterGCNAE}
W.-L. Chiang, X.~Liu, S.~Si, Y.~Li, S.~Bengio, and C.~Hsieh, ``Cluster-{{GCN}}: An efficient algorithm for training deep and large graph convolutional networks,'' \emph{Proceedings of the 25th ACM SIGKDD International Conference on Knowledge Discovery \& Data Mining}, 2019.

\bibitem{graphsaint-iclr20}
\BIBentryALTinterwordspacing
H.~Zeng, H.~Zhou, A.~Srivastava, R.~Kannan, and V.~Prasanna, ``{GraphSAINT}: Graph sampling based inductive learning method,'' in \emph{International Conference on Learning Representations}, 2020. [Online]. Available: \url{https://openreview.net/forum?id=BJe8pkHFwS}
\BIBentrySTDinterwordspacing

\bibitem{zu2019survey}
Z.~{Wu}, S.~{Pan}, F.~{Chen}, G.~{Long}, C.~{Zhang}, and P.~S. {Yu}, ``A comprehensive survey on graph neural networks,'' \emph{IEEE Transactions on Neural Networks and Learning Systems}, pp. 1--21, 2020.

\bibitem{StellarGraph}
\BIBentryALTinterwordspacing
{CSIRO's Data61}, ``Stellargraph machine learning library,'' 2018. [Online]. Available: \url{https://github.com/stellargraph/stellargraph}
\BIBentrySTDinterwordspacing

\bibitem{grattarola2020graph}
D.~Grattarola and C.~Alippi, ``Graph neural networks in tensorflow and keras with spektral,'' \emph{arXiv preprint arXiv:2006.12138}, 2020.

\bibitem{sign_icml_grl2020}
F.~Frasca, E.~Rossi, D.~Eynard, B.~Chamberlain, M.~Bronstein, and F.~Monti, ``{{SIGN}}: Scalable inception graph neural networks,'' in \emph{ICML 2020 Workshop on Graph Representation Learning and Beyond}, 2020.

\bibitem{graphsaint-ipdps19}
H.~Zeng, H.~Zhou, A.~Srivastava, R.~Kannan, and V.~Prasanna, ``Accurate, efficient and scalable graph embedding,'' in \emph{2019 IEEE International Parallel and Distributed Processing Symposium (IPDPS)}, May 2019.

\bibitem{Jia2020ImprovingTA}
Z.~Jia, S.~Lin, M.~Gao, M.~Zaharia, and A.~Aiken, ``Improving the accuracy, scalability, and performance of graph neural networks with {{Roc}},'' in \emph{MLSys}, 2020.

\bibitem{ogbLeaderboard}
\BIBentryALTinterwordspacing
{Open Graph Benchmark}, ``Leaderboards for node property prediction | open graph benchmark,'' 2021. [Online]. Available: \url{https://ogb.stanford.edu/docs/leader_nodeprop/}
\BIBentrySTDinterwordspacing

\bibitem{SAGA}
L.~Ma, Z.~Yang, Y.~Miao, J.~Xue, M.~Wu, L.~Zhou, and Y.~Dai, ``Towards efficient large-scale graph neural network computing,'' \emph{ArXiv}, vol. abs/1810.08403, 2018.

\bibitem{10.14778/3384345.3384358}
\BIBentryALTinterwordspacing
P.~Gera, H.~Kim, P.~Sao, H.~Kim, and D.~Bader, ``Traversing large graphs on {{GPUs}} with unified memory,'' \emph{Proc. VLDB Endow.}, vol.~13, no.~7, p. 1119–1133, Mar. 2020. [Online]. Available: \url{https://doi.org/10.14778/3384345.3384358}
\BIBentrySTDinterwordspacing

\bibitem{10.1145/3342195.3387537}
\BIBentryALTinterwordspacing
A.~H.~N. Sabet, Z.~Zhao, and R.~Gupta, ``Subway: Minimizing data transfer during out-of-{{GPU}}-memory graph processing,'' in \emph{Proceedings of the Fifteenth European Conference on Computer Systems}, ser. EuroSys '20.\hskip 1em plus 0.5em minus 0.4em\relax New York, NY, USA: Association for Computing Machinery, 2020. [Online]. Available: \url{https://doi.org/10.1145/3342195.3387537}
\BIBentrySTDinterwordspacing

\bibitem{P100Whitepaper}
\BIBentryALTinterwordspacing
Nvidia, ``Nvidia {{Tesla}} {{P100}} whitepaper,'' 2016. [Online]. Available: \url{https://images.nvidia.com/content/pdf/tesla/whitepaper/pascal-architecture-whitepaper.pdf}
\BIBentrySTDinterwordspacing

\bibitem{A100Whitepaper}
\BIBentryALTinterwordspacing
Nvidia, ``Nvidia {{A100}} {{TensorCore}} {{GPU}} architecture whitepaper,'' 2020. [Online]. Available: \url{https://www.nvidia.com/content/dam/en-zz/Solutions/Data-Center/nvidia-ampere-architecture-whitepaper.pdf}
\BIBentrySTDinterwordspacing

\bibitem{UVMPrimer}
\BIBentryALTinterwordspacing
M.~Ujaldón, ``Unified memory,'' 2015. [Online]. Available: \url{http://gpu.cs.uct.ac.za/Slides/unified-and-3D-memory.pdf}
\BIBentrySTDinterwordspacing

\bibitem{pearson19}
\BIBentryALTinterwordspacing
C.~Pearson, A.~Dakkak, S.~Hashash, C.~Li, I.-H. Chung, J.~Xiong, and W.-M. Hwu, ``Evaluating characteristics of {{CUDA}} communication primitives on high-bandwidth interconnects,'' in \emph{Proceedings of the 2019 ACM/SPEC International Conference on Performance Engineering}, ser. ICPE '19.\hskip 1em plus 0.5em minus 0.4em\relax New York, NY, USA: Association for Computing Machinery, 2019. [Online]. Available: \url{https://doi.org/10.1145/3297663.3310299} p. 209–218.
\BIBentrySTDinterwordspacing

\bibitem{relTransfer1}
Y.~Fujii, T.~Azumi, N.~Nishio, S.~Kato, and M.~Edahiro, ``Data transfer matters for {{GPU}} computing,'' in \emph{Proceedings of the International Conference on Parallel and Distributed Systems - ICPADS}, 12 2013, pp. 275--282.

\bibitem{gpudirectrdma}
\BIBentryALTinterwordspacing
Nvidia, ``Developing a linux kernel module using {{GPUDirect}} {{RDMA}},'' 2021. [Online]. Available: \url{https://docs.nvidia.com/cuda/gpudirect-rdma/index.html}
\BIBentrySTDinterwordspacing

\bibitem{UVMPrimer2}
\BIBentryALTinterwordspacing
M.~Harris, ``Unified memory for {{CUDA}} beginners,'' 2017. [Online]. Available: \url{https://developer.nvidia.com/blog/unified-memory-cuda-beginners/}
\BIBentrySTDinterwordspacing

\bibitem{Chien_2019}
\BIBentryALTinterwordspacing
S.~Chien, I.~Peng, and S.~Markidis, ``Performance evaluation of advanced features in {{CUDA}} unified memory,'' \emph{2019 IEEE/ACM Workshop on Memory Centric High Performance Computing (MCHPC)}, Nov 2019. [Online]. Available: \url{http://dx.doi.org/10.1109/MCHPC49590.2019.00014}
\BIBentrySTDinterwordspacing

\bibitem{krizhevsky2012imagenet}
A.~Krizhevsky, I.~Sutskever, and G.~E. Hinton, ``{{ImageNet}} classification with deep convolutional neural networks,'' in \emph{Advances in neural information processing systems}, 2012, pp. 1097--1105.

\bibitem{He2015}
K.~He, X.~Zhang, S.~Ren, and J.~Sun, ``Deep residual learning for image recognition,'' \emph{arXiv preprint arXiv:1512.03385}, 2015.

\bibitem{attention2018graph}
\BIBentryALTinterwordspacing
P.~Veličković, G.~Cucurull, A.~Casanova, A.~Romero, P.~Liò, and Y.~Bengio, ``Graph attention networks,'' in \emph{International Conference on Learning Representations}, 2018. [Online]. Available: \url{https://openreview.net/forum?id=rJXMpikCZ}
\BIBentrySTDinterwordspacing

\bibitem{torchvision}
\BIBentryALTinterwordspacing
S.~Marcel and Y.~Rodriguez, ``Torchvision the machine-vision package of torch,'' in \emph{Proceedings of the 18th ACM International Conference on Multimedia}, ser. MM '10.\hskip 1em plus 0.5em minus 0.4em\relax New York, NY, USA: Association for Computing Machinery, 2010. [Online]. Available: \url{https://doi.org/10.1145/1873951.1874254} p. 1485–1488.
\BIBentrySTDinterwordspacing

\bibitem{uvmguide}
\BIBentryALTinterwordspacing
Nvidia, ``Beyond {{GPU}} memory limits with unified memory on pascal,'' 2016. [Online]. Available: \url{https://developer.nvidia.com/blog/beyond-gpu-memory-limits-unified-memory-pascal/}
\BIBentrySTDinterwordspacing

\bibitem{edwardz.yangPytorchTorchCsrc2017}
\BIBentryALTinterwordspacing
{Edward Z. Yang}, ``Autograd - pytorch/pytorch,'' 2017. [Online]. Available: \url{https://github.com/pytorch/pytorch/blob/main/torch/csrc/autograd/README.md}
\BIBentrySTDinterwordspacing

\bibitem{edwardz.yangATenAtenSrc2018}
\BIBentryALTinterwordspacing
{Edward Z. Yang}, ``Aten/aten/src/readme.md at master · zdevito/aten · {{GitHub}},'' 2018. [Online]. Available: \url{https://github.com/zdevito/ATen/blob/master/aten/src/README.md}
\BIBentrySTDinterwordspacing

\bibitem{BoVWFI}
P.~Boldi and S.~Vigna, ``The {W}eb{G}raph framework {I}: {C}ompression techniques,'' in \emph{Proceedings of the Thirteenth International World Wide Web Conference (WWW 2004)}.\hskip 1em plus 0.5em minus 0.4em\relax Manhattan, USA: ACM Press, 2004, pp. 595--601.

\bibitem{Kwak10www}
H.~Kwak, C.~Lee, H.~Park, and S.~Moon, ``{W}hat is {T}witter, a social network or a news media?'' in \emph{WWW '10: Proc. the 19th Intl. Conf. on World Wide Web}.\hskip 1em plus 0.5em minus 0.4em\relax New York, NY, USA: ACM, 2010, pp. 591--600.

\bibitem{konect}
\BIBentryALTinterwordspacing
J.~Kunegis, ``Konect: The koblenz network collection,'' in \emph{Proceedings of the 22nd International Conference on World Wide Web}, ser. WWW '13 Companion.\hskip 1em plus 0.5em minus 0.4em\relax New York, NY, USA: Association for Computing Machinery, 2013. [Online]. Available: \url{https://doi.org/10.1145/2487788.2488173} p. 1343–1350.
\BIBentrySTDinterwordspacing

\bibitem{liuWinnerTakeAllColumnRow2023}
\BIBentryALTinterwordspacing
Z.~Liu, G.~Wang, S.~Zhong, Z.~Xu, D.~Zha, R.~Tang, Z.~Jiang, K.~Zhou, V.~Chaudhary, S.~Xu, and X.~Hu, ``Winner-take-all column row sampling for memory efficient adaptation of language model,'' Dec. 2023. [Online]. Available: \url{https://arxiv.org/abs/2305.15265}
\BIBentrySTDinterwordspacing

\bibitem{korthikantiReducingActivationRecomputation2022}
\BIBentryALTinterwordspacing
V.~Korthikanti, J.~Casper, S.~Lym, L.~McAfee, M.~Andersch, M.~Shoeybi, and B.~Catanzaro, ``Reducing activation recomputation in large transformer models,'' \emph{arXiv preprint}, no. 2205.05198, May 2022. [Online]. Available: \url{https://arxiv.org/abs/2205.05198}
\BIBentrySTDinterwordspacing

\bibitem{jiangMegaScaleScalingLarge2024}
\BIBentryALTinterwordspacing
Z.~Jiang, H.~Lin, Y.~Zhong, Q.~Huang, Y.~Chen, Z.~Zhang, Y.~Peng, X.~Li, C.~Xie, S.~Nong, Y.~Jia, S.~He, H.~Chen, Z.~Bai, Q.~Hou, S.~Yan, D.~Zhou, Y.~Sheng, Z.~Jiang, H.~Xu, H.~Wei, Z.~Zhang, P.~Nie, L.~Zou, S.~Zhao, L.~Xiang, Z.~Liu, Z.~Li, X.~Jia, J.~Ye, X.~Jin, and X.~Liu, ``{{MegaScale}}: Scaling large language model training to more than 10,000 {{GPUs}},'' \emph{arXiv preprint}, no. 2402.15627, Feb. 2024. [Online]. Available: \url{https://arxiv.org/abs/2402.15627}
\BIBentrySTDinterwordspacing

\bibitem{workshopBLOOM176BParameterOpenAccess2023}
\BIBentryALTinterwordspacing
T.~L. Scao, A.~Fan, C.~Akiki, E.~Pavlick, S.~Ili{\'c}, D.~Hesslow, R.~Castagn{\'e}, A.~S. Luccioni, F.~Yvon, M.~Gall{\'e}, J.~Tow, A.~M. Rush, S.~Biderman, A.~Webson, P.~S. Ammanamanchi, T.~Wang, B.~Sagot, N.~Muennighoff, A.~V. {del Moral}, O.~Ruwase, R.~Bawden, S.~Bekman, A.~{McMillan-Major}, I.~Beltagy, H.~Nguyen, L.~Saulnier, S.~Tan, P.~O. Suarez, V.~Sanh, H.~Lauren{\c c}on, Y.~Jernite, J.~Launay, M.~Mitchell, C.~Raffel, A.~Gokaslan, A.~Simhi, A.~Soroa, A.~F. Aji, A.~Alfassy, A.~Rogers, A.~K. Nitzav, C.~Xu, C.~Mou, C.~Emezue, C.~Klamm, C.~Leong, D.~{van Strien}, D.~I. Adelani, D.~Radev, E.~G. Ponferrada, E.~Levkovizh, E.~Kim, E.~B. Natan, F.~De~Toni, G.~Dupont, G.~Kruszewski, G.~Pistilli, H.~Elsahar, H.~Benyamina, H.~Tran, I.~Yu, I.~Abdulmumin, I.~Johnson, I.~{Gonzalez-Dios}, J.~{de la Rosa}, J.~Chim, J.~Dodge, J.~Zhu, J.~Chang, J.~Frohberg, J.~Tobing, J.~Bhattacharjee, K.~Almubarak, K.~Chen, K.~Lo, L.~Von~Werra, L.~Weber, L.~Phan, L.~B. {allal}, L.~Tanguy, M.~Dey, M.~R. Mu{\~n}oz, M.~Masoud, M.~Grandury,
  M.~{\v S}a{\v s}ko, M.~Huang, M.~Coavoux, M.~Singh, M.~T.-J. Jiang, M.~C. Vu, M.~A. Jauhar, M.~Ghaleb, N.~Subramani, N.~Kassner, N.~Khamis, O.~Nguyen, O.~Espejel, O.~{de Gibert}, P.~Villegas, P.~Henderson, P.~Colombo, P.~Amuok, Q.~Lhoest, R.~Harliman, R.~Bommasani, R.~L. L{\'o}pez, R.~Ribeiro, S.~Osei, S.~Pyysalo, S.~Nagel, S.~Bose, S.~H. Muhammad, S.~Sharma, S.~Longpre, S.~Nikpoor, S.~Silberberg, S.~Pai, S.~Zink, T.~T. Torrent, T.~Schick, T.~Thrush, V.~Danchev, V.~Nikoulina, V.~Laippala, V.~Lepercq, V.~Prabhu, Z.~Alyafeai, Z.~Talat, A.~Raja, B.~Heinzerling, C.~Si, D.~E. Ta{\c s}ar, E.~Salesky, S.~J. Mielke, W.~Y. Lee, A.~Sharma, A.~Santilli, A.~Chaffin, A.~Stiegler, D.~Datta, E.~Szczechla, G.~Chhablani, H.~Wang, H.~Pandey, H.~Strobelt, J.~A. Fries, J.~Rozen, L.~Gao, L.~Sutawika, M.~S. Bari, M.~S. {Al-shaibani}, M.~Manica, N.~Nayak, R.~Teehan, S.~Albanie, S.~Shen, S.~{Ben-David}, S.~H. Bach, T.~Kim, T.~Bers, T.~Fevry, T.~Neeraj, U.~Thakker, V.~Raunak, X.~Tang, Z.-X. Yong, Z.~Sun, S.~Brody, Y.~Uri,
  H.~Tojarieh, A.~Roberts, H.~W. Chung, J.~Tae, J.~Phang, O.~Press, C.~Li, D.~Narayanan, H.~Bourfoune, J.~Casper, J.~Rasley, M.~Ryabinin, M.~Mishra, M.~Zhang, M.~Shoeybi, M.~Peyrounette, N.~Patry, N.~Tazi, O.~Sanseviero, P.~{von Platen}, P.~Cornette, P.~F. Lavall{\'e}e, R.~Lacroix, S.~Rajbhandari, S.~Gandhi, S.~Smith, S.~Requena, S.~Patil, T.~Dettmers, A.~Baruwa, A.~Singh, A.~Cheveleva, A.-L. Ligozat, A.~Subramonian, A.~N{\'e}v{\'e}ol, C.~Lovering, D.~Garrette, D.~Tunuguntla, E.~Reiter, E.~Taktasheva, E.~Voloshina, E.~Bogdanov, G.~I. Winata, H.~Schoelkopf, J.-C. Kalo, J.~Novikova, J.~Z. Forde, J.~Clive, J.~Kasai, K.~Kawamura, L.~Hazan, M.~Carpuat, M.~Clinciu, N.~Kim, N.~Cheng, O.~Serikov, O.~Antverg, O.~{van der Wal}, R.~Zhang, R.~Zhang, S.~Gehrmann, S.~Mirkin, S.~Pais, T.~Shavrina, T.~Scialom, T.~Yun, T.~Limisiewicz, V.~Rieser, V.~Protasov, V.~Mikhailov, Y.~Pruksachatkun, Y.~Belinkov, Z.~Bamberger, Z.~Kasner, A.~Rueda, A.~Pestana, A.~Feizpour, A.~Khan, A.~Faranak, A.~Santos, A.~Hevia, A.~Unldreaj,
  A.~Aghagol, A.~Abdollahi, A.~Tammour, A.~HajiHosseini, B.~Behroozi, B.~Ajibade, B.~Saxena, C.~M. Ferrandis, D.~McDuff, D.~Contractor, D.~Lansky, D.~David, D.~Kiela, D.~A. Nguyen, E.~Tan, E.~Baylor, E.~Ozoani, F.~Mirza, F.~Ononiwu, H.~Rezanejad, H.~Jones, I.~Bhattacharya, I.~Solaiman, I.~Sedenko, I.~Nejadgholi, J.~Passmore, J.~Seltzer, J.~B. Sanz, L.~Dutra, M.~Samagaio, M.~Elbadri, M.~Mieskes, M.~Gerchick, M.~Akinlolu, M.~McKenna, M.~Qiu, M.~Ghauri, M.~Burynok, N.~Abrar, N.~Rajani, N.~Elkott, N.~Fahmy, O.~Samuel, R.~An, R.~Kromann, R.~Hao, S.~Alizadeh, S.~Shubber, S.~Wang, S.~Roy, S.~Viguier, T.~Le, T.~Oyebade, T.~Le, Y.~Yang, Z.~Nguyen, A.~R. Kashyap, A.~Palasciano, A.~Callahan, A.~Shukla, A.~{Miranda-Escalada}, A.~Singh, B.~Beilharz, B.~Wang, C.~Brito, C.~Zhou, C.~Jain, C.~Xu, C.~Fourrier, D.~L. Peri{\~n}{\'a}n, D.~Molano, D.~Yu, E.~Manjavacas, F.~Barth, F.~Fuhrimann, G.~Altay, G.~Bayrak, G.~Burns, H.~U. Vrabec, I.~Bello, I.~Dash, J.~Kang, J.~Giorgi, J.~Golde, J.~D. Posada, K.~R. Sivaraman, L.~Bulchandani,
  L.~Liu, L.~Shinzato, M.~H. {de Bykhovetz}, M.~Takeuchi, M.~P{\`a}mies, M.~A. Castillo, M.~Nezhurina, M.~S{\"a}nger, M.~Samwald, M.~Cullan, M.~Weinberg, M.~De~Wolf, M.~Mihaljcic, M.~Liu, M.~Freidank, M.~Kang, N.~Seelam, N.~Dahlberg, N.~M. Broad, N.~Muellner, P.~Fung, P.~Haller, R.~Chandrasekhar, R.~Eisenberg, R.~Martin, R.~Canalli, R.~Su, R.~Su, S.~Cahyawijaya, S.~Garda, S.~S. Deshmukh, S.~Mishra, S.~Kiblawi, S.~Ott, S.~{Sang-aroonsiri}, S.~Kumar, S.~Schweter, S.~Bharati, T.~Laud, T.~Gigant, T.~Kainuma, W.~Kusa, Y.~Labrak, Y.~S. Bajaj, Y.~Venkatraman, Y.~Xu, Y.~Xu, Y.~Xu, Z.~Tan, Z.~Xie, Z.~Ye, M.~Bras, Y.~Belkada, and T.~Wolf, ``{{BLOOM}}: A 176b-parameter open-access multilingual language model,'' \emph{arXiv preprint}, no. 2211.05100, June 2023. [Online]. Available: \url{https://arxiv.org/abs/2211.05100}
\BIBentrySTDinterwordspacing

\bibitem{DissectingBatchingEffects}
\BIBentryALTinterwordspacing
L.~Chen, ``Dissecting batching effects in {{GPT}} inference,'' 2023, accessed 07/21/2024. [Online]. Available: \url{https://le.qun.ch/en/blog/2023/05/13/transformer-batching/}
\BIBentrySTDinterwordspacing

\bibitem{anthonyCaseCoDesigningModel2024}
\BIBentryALTinterwordspacing
Q.~Anthony, J.~Hatef, D.~Narayanan, S.~Biderman, S.~Bekman, J.~Yin, A.~Shafi, H.~Subramoni, and D.~Panda, ``The case for co-designing model architectures with hardware,'' \emph{arXiv preprint}, no. 2401.14489, Jan. 2024. [Online]. Available: \url{https://arxiv.org/abs/2401.14489}
\BIBentrySTDinterwordspacing

\bibitem{aminabadiDeepSpeedInferenceEnabling2022}
\BIBentryALTinterwordspacing
R.~Y. Aminabadi, S.~Rajbhandari, M.~Zhang, A.~A. Awan, C.~Li, D.~Li, E.~Zheng, J.~Rasley, S.~Smith, O.~Ruwase, and Y.~He, ``{{DeepSpeed Inference}}: Enabling efficient inference of transformer models at unprecedented scale,'' \emph{arXiv preprint}, no. 2207.00032, June 2022. [Online]. Available: \url{https://arxiv.org/abs/2207.00032}
\BIBentrySTDinterwordspacing

\bibitem{kaplanScalingLawsNeural2020}
\BIBentryALTinterwordspacing
J.~Kaplan, S.~McCandlish, T.~Henighan, T.~B. Brown, B.~Chess, R.~Child, S.~Gray, A.~Radford, J.~Wu, and D.~Amodei, ``Scaling laws for neural language models,'' \emph{arXiv preprint}, no. 2001.08361, Jan. 2020. [Online]. Available: \url{https://arxiv.org/abs/2001.08361}
\BIBentrySTDinterwordspacing

\bibitem{mccandlishEmpiricalModelLargeBatch2018}
\BIBentryALTinterwordspacing
S.~McCandlish, J.~Kaplan, D.~Amodei, and O.~D. Team, ``An empirical model of large-batch training,'' \emph{arXiv preprint}, no. 1812.06162, Dec. 2018. [Online]. Available: \url{https://arxiv.org/abs/1812.06162}
\BIBentrySTDinterwordspacing

\bibitem{chenTrainingDeepNets2016}
\BIBentryALTinterwordspacing
T.~Chen, B.~Xu, C.~Zhang, and C.~Guestrin, ``Training deep nets with sublinear memory cost,'' \emph{arXiv preprint}, no. 1604.06174, Apr. 2016. [Online]. Available: \url{https://arxiv.org/abs/1604.06174}
\BIBentrySTDinterwordspacing

\bibitem{theepochaiAnnouncingEpochAI2023}
\BIBentryALTinterwordspacing
{Epoch AI}, ``Announcing {{Epoch AI}}'s updated parameter, compute and data trends database,'' Oct. 2023, accessed 07/21/2024. [Online]. Available: \url{https://epochai.org/blog/announcing-updated-pcd-database}
\BIBentrySTDinterwordspacing

\bibitem{microsoftNDA100V4series2024}
\BIBentryALTinterwordspacing
{Microsoft}, ``{{ND A100}} {{V4}}-series - {{Azure}} virtual machines,'' Feb. 2024, accessed 07/21/2024. [Online]. Available: \url{https://learn.microsoft.com/en-us/azure/virtual-machines/nda100-v4-series}
\BIBentrySTDinterwordspacing

\bibitem{googleGPUMachineTypes}
\BIBentryALTinterwordspacing
{Google}, ``{{GPU}} machine types {\textbar} compute engine documentation,'' 2017, accessed 07/21/2024. [Online]. Available: \url{https://cloud.google.com/compute/docs/gpus}
\BIBentrySTDinterwordspacing

\bibitem{ncsaDeltaProjectProfile}
\BIBentryALTinterwordspacing
{NCSA}, ``Delta project profile,'' 2022, accessed 07/21/2024. [Online]. Available: \url{https://www.ncsa.illinois.edu/research/project-highlights/delta/}
\BIBentrySTDinterwordspacing

\bibitem{kamahoriFiddlerCPUGPUOrchestration2024}
\BIBentryALTinterwordspacing
K.~Kamahori, Y.~Gu, K.~Zhu, and B.~Kasikci, ``Fiddler: {{CPU}}-{{GPU}} orchestration for fast inference of mixture-of-experts models,'' 2024. [Online]. Available: \url{http://arxiv.org/abs/2402.07033}
\BIBentrySTDinterwordspacing

\bibitem{renZeROOffloadDemocratizingBillionScale2021}
J.~Ren, S.~Rajbhandari, R.~Y. Aminabadi, S.~Yang, M.~Zhang, D.~Li, O.~Ruwase, and Y.~He, ``{{ZeRO-Offload}}: Democratizing billion-scale model training,'' in \emph{Proceedings of the 2021 {{USENIX Annual Technical Conference}}}, 2021.

\bibitem{songPowerInferFastLarge2023}
\BIBentryALTinterwordspacing
Y.~Song, Z.~Mi, H.~Xie, and H.~Chen, ``{{PowerInfer}}: Fast large language model serving with a consumer-grade {{GPU}},'' 2023. [Online]. Available: \url{http://arxiv.org/abs/2312.12456}
\BIBentrySTDinterwordspacing

\bibitem{baeFlashNeuronSSDEnabledLargeBatch2021}
J.~Bae, J.~Lee, Y.~Jin, S.~Son, S.~Kim, T.~J. Ham, J.~W. Lee, and H.~Jang, ``{{FlashNeuron}}: {{SSD}}-enabled large-batch training of very deep neural networks,'' in \emph{Proceedings of the 19th {{USENIX Conference}} on {{File}} and {{Storage Technologies}}}, 2021.

\bibitem{googleGoogleCloudHyperdisk}
\BIBentryALTinterwordspacing
Google, ``About {{Google}} cloud hyperdisk | compute engine documentation,'' 2022, accessed 07/30/2024. [Online]. Available: \url{https://cloud.google.com/compute/docs/disks/hyperdisks}
\BIBentrySTDinterwordspacing

\bibitem{lockwoodArchitecturePerformancePerlmutter2024}
\BIBentryALTinterwordspacing
G.~K. Lockwood, A.~Chiusole, L.~Gerhardt, K.~Lozinskiy, D.~Paul, and N.~J. Wright, ``Architecture and performance of {{Perlmutter}}'s 35 {{PB ClusterStor E1000}} all‐flash file system,'' \emph{Concurrency and Computation: Practice and Experience}, p. e8143, 2024. [Online]. Available: \url{https://onlinelibrary.wiley.com/doi/10.1002/cpe.8143}
\BIBentrySTDinterwordspacing

\bibitem{microsoftMicrosoftMegatronDeepSpeedOngoing2019}
\BIBentryALTinterwordspacing
{Microsoft}, ``{{Megatron-DeepSpeed}}: Ongoing research training transformer language models at scale, including: {{BERT}} \& {{GPT-2}},'' 2019. [Online]. Available: \url{https://github.com/microsoft/Megatron-DeepSpeed}
\BIBentrySTDinterwordspacing

\bibitem{jordanhoffmannTrainingComputeOptimalLarge2022}
\BIBentryALTinterwordspacing
J.~Hoffmann, S.~Borgeaud, A.~Mensch, E.~Buchatskaya, T.~Cai, E.~Rutherford, D.~de~Las~Casas, L.~A. Hendricks, J.~Welbl, A.~Clark, T.~Hennigan, E.~Noland, K.~Millican, G.~Driessche, B.~Damoc, A.~Guy, S.~Osindero, K.~Simonyan, E.~Elsen, J.~W. Rae, O.~Vinyals, and L.~Sifre, ``Training compute-optimal large language models,'' 2022. [Online]. Available: \url{http://arxiv.org/abs/2203.15556}
\BIBentrySTDinterwordspacing

\bibitem{brown2020languagemodelsfewshotlearners}
\BIBentryALTinterwordspacing
T.~B. Brown, B.~Mann, N.~Ryder, M.~Subbiah, J.~Kaplan, P.~Dhariwal, A.~Neelakantan, P.~Shyam, G.~Sastry, A.~Askell, S.~Agarwal, A.~Herbert-Voss, G.~Krueger, T.~Henighan, R.~Child, A.~Ramesh, D.~M. Ziegler, J.~Wu, C.~Winter, C.~Hesse, M.~Chen, E.~Sigler, M.~Litwin, S.~Gray, B.~Chess, J.~Clark, C.~Berner, S.~McCandlish, A.~Radford, I.~Sutskever, and D.~Amodei, ``Language models are few-shot learners,'' 2020. [Online]. Available: \url{https://arxiv.org/abs/2005.14165}
\BIBentrySTDinterwordspacing

\bibitem{specAllSPECOSG2024}
\BIBentryALTinterwordspacing
{SPEC}, ``All {{SPEC}}/{{OSG}} results.'' [Online]. Available: \url{http://spec.org/cgi-bin/osgresults?conf=cpu2017;op=dump;format=csvdump}
\BIBentrySTDinterwordspacing

\bibitem{samsungUltraLowLatencySamsung2017}
\BIBentryALTinterwordspacing
{Samsung}, ``Ultra-low latency with {{Samsung Z-NAND SSD}},'' 2017, accessed 07/30/2024. [Online]. Available: \url{https://download.semiconductor.samsung.com/resources/brochure/Ultra-Low Latency with Samsung Z-NAND SSD.pdf}
\BIBentrySTDinterwordspacing

\bibitem{jedecsolidstatetechnologyassociationJESD218BSolidStateDrive2016}
\BIBentryALTinterwordspacing
\emph{{{JESD218B}}: Solid-State Drive ({{SSD}}) Requirements and Endurance Test Method}, {JEDEC SOLID STATE TECHNOLOGY ASSOCIATION} Std., 2016. [Online]. Available: \url{https://www.jedec.org/sites/default/files/docs/JESD218B.pdf}
\BIBentrySTDinterwordspacing

\bibitem{lenovoWhatNeedKnow2023}
\BIBentryALTinterwordspacing
{Lenovo}, ``What do {{I}} need to know about {{SSD}} endurance and overprovisioning?'' 2023. [Online]. Available: \url{https://thinksystem.lenovofiles.com/storage/help/index.jsp?topic=%2Fde-series-olh-11.80%2Fwhat-do-i-need-to-know-about-ssd-endurance-and-overprovisioning.html}
\BIBentrySTDinterwordspacing

\bibitem{qnapsystemsinc.QNAPNASSolution2108}
\BIBentryALTinterwordspacing
{QNAP Systems}, ``{{QNAP}} {{NAS}} solution: {{QTS}} {{SSD}} extra over-provisioning,'' 2018. [Online]. Available: \url{https://anfatech.com.vn/wp-content/uploads/2021/03/ssd-over-provisioning.pdf}
\BIBentrySTDinterwordspacing

\bibitem{smartmodulartechnologiesinc.WhySMARTOverProvisioning2024}
\BIBentryALTinterwordspacing
{SMART Modular Technologies, Inc.}, ``Why {{SMART}}'s over-provisioning?'' 2024. [Online]. Available: \url{https://www.smartm.com/technology/over-provisioning}
\BIBentrySTDinterwordspacing

\bibitem{solidigmSolidigmSSDEndurance}
\BIBentryALTinterwordspacing
{Solidigm}, ``{{Solidigm}}™ {{SSD}} endurance estimator,'' 2022. [Online]. Available: \url{https://estimator.solidigm.com/ssdendurance/index.htm}
\BIBentrySTDinterwordspacing

\bibitem{intelOverProvisioningNANDBasedIntel2018}
\BIBentryALTinterwordspacing
{Intel}, ``Over-provisioning {{NAND}}-based {{Intel}}® {{SSDs}} for better endurance,'' 2018. [Online]. Available: \url{https://www.ioncomputer.com/ion/body/documents/over-provisioning-nand-based-ssds-better-endurance-whitepaper.pdf}
\BIBentrySTDinterwordspacing

\bibitem{samsungOverProvisioningBenefitsSamsung2019}
\BIBentryALTinterwordspacing
{Samsung}, ``Over-provisioning benefits for {{Samsung}} data center {{SSDs}},'' 2019. [Online]. Available: \url{https://download.semiconductor.samsung.com/resources/white-paper/S190311-SAMSUNG-Memory-Over-Provisioning-White-paper.pdf}
\BIBentrySTDinterwordspacing

\bibitem{maneasOperationalCharacteristicsSSDs2022}
S.~Maneas, K.~Mahdaviani, T.~Emami, and B.~Schroeder, ``Operational characteristics of {{SSDs}} in enterprise storage systems: A large-scale field study,'' in \emph{Proceedings of the 20th {{USENIX Conference}} on {{File}} and {{Storage Technologies}}}, 2022.

\bibitem{solidigmD7P5620MidEndurancePCIe2023}
\BIBentryALTinterwordspacing
{Solidigm}, ``D7-{{P5620}} mid-endurance {{PCIe}} 4.0 {{NVMe SSD}} for data centers {\textbar} {{Solidigm D7 SSD}},'' 2023, accessed 07/21/2024. [Online]. Available: \url{https://www.solidigm.com/products/data-center/d7/p5620.html}
\BIBentrySTDinterwordspacing

\bibitem{solidigmD7P58102023}
\BIBentryALTinterwordspacing
{Solidigm}, ``D7-{{P5810}},'' 2023, accessed 07/21/2024. [Online]. Available: \url{https://www.solidigm.com/products/data-center/d7/p5810.html}
\BIBentrySTDinterwordspacing

\bibitem{neweggSolidigmSolidState2024}
\BIBentryALTinterwordspacing
{Newegg}, ``{{Solidigm}}™ solid state drive {{D7-P5620}} series (12.{{8TB}}, {{U.2}} 15mm, 2.5", {{PCIe}} 4.0 x4, {{3D4}}, {{TLC}}) generic no {{OPAL}} single pack data center / server / internal {{SSD}} (ssdpf2ke128t1n1) - newegg.com,'' 2024, accessed 07/21/2024. [Online]. Available: \url{https://www.newegg.com/solidigm-12-8-tb-d7-p5620-series/p/N82E16820318023}
\BIBentrySTDinterwordspacing

\bibitem{kioxiaFL6Series5inch2022}
\BIBentryALTinterwordspacing
{KIOXIA}, ``{{FL6}} series (2.5-inch) {\textbar} {{KIOXIA}} - {{United States}} ({{English}}),'' 2022, accessed 07/21/2024. [Online]. Available: \url{https://americas.kioxia.com/en-us/business/ssd/enterprise-ssd/fl6.html}
\BIBentrySTDinterwordspacing

\bibitem{serverorbitKioxiaFL6XHUL1T606TB2024}
\BIBentryALTinterwordspacing
{ServerOrbit}, ``Kioxia fl6xhul1t60 1.{{6TB PCIe4 NVMe SSD}} brand new,'' 2024, accessed 07/21/2024. [Online]. Available: \url{https://serverorbit.com/buy-kioxia-fl6xhul1t60-1-6tb-pcie4-nvme-ssd/}
\BIBentrySTDinterwordspacing

\bibitem{dihuniSOLIDIGMSSDPF2SQ800GZ01D7P58102024}
\BIBentryALTinterwordspacing
{Dihuni}, ``{{SOLIDIGM}} ssdpf2sq800gz01 {{D7-P5810}} solid state drive -- {{Dihuni}} -- {{GPU}} server for {{AI}}, data center \& {{IoT}} hardware \& software solutions,'' 2024, accessed 07/21/2024. [Online]. Available: \url{https://www.dihuni.com/product/solidigm-ssdpf2sq800gz01-d7-p5810-solid-state-drive/}
\BIBentrySTDinterwordspacing

\bibitem{inupakutikaQuantifyingPerformanceGains2022}
\BIBentryALTinterwordspacing
D.~Inupakutika, B.~Davis, Q.~Yang, D.~Kim, and D.~Akopian, ``Quantifying performance gains of {{GPUDirect Storage}},'' in \emph{2022 {{IEEE International Conference}} on {{Networking}}, {{Architecture}} and {{Storage}} ({{NAS}})}, 2022. [Online]. Available: \url{https://ieeexplore.ieee.org/document/9925516} pp. 1--9.
\BIBentrySTDinterwordspacing

\bibitem{sunSTRONGHOLDFastAffordable2022}
\BIBentryALTinterwordspacing
X.~Sun, W.~Wang, S.~Qiu, R.~Yang, S.~Huang, J.~Xu, and Z.~Wang, ``{{STRONGHOLD}}: Fast and affordable billion-scale deep learning model training,'' in \emph{{{SC22}}: {{International Conference}} for {{High Performance Computing}}, {{Networking}}, {{Storage}} and {{Analysis}}}, 2022. [Online]. Available: \url{https://ieeexplore.ieee.org/document/10046110} pp. 1--17.
\BIBentrySTDinterwordspacing

\bibitem{rajbhandariZeROinfinityBreakingGPU2021}
S.~Rajbhandari, O.~Ruwase, J.~Rasley, S.~Smith, and Y.~He, ``{{ZeRO-Infinity}}: Breaking the {{GPU}} memory wall for extreme scale deep learning,'' in \emph{Proceedings of the {{International Conference}} for {{High Performance Computing}}, {{Networking}}, {{Storage}} and {{Analysis}}}.\hskip 1em plus 0.5em minus 0.4em\relax St. Louis Missouri: ACM, Nov. 2021, pp. 1--14.

\bibitem{shengFlexGenHighThroughputGenerative2023}
\BIBentryALTinterwordspacing
Y.~Sheng, L.~Zheng, B.~Yuan, Z.~Li, M.~Ryabinin, D.~Y. Fu, Z.~Xie, B.~Chen, C.~Barrett, J.~E. Gonzalez, P.~Liang, C.~R{\'e}, I.~Stoica, and C.~Zhang, ``{{FlexGen}}: High-throughput generative inference of large language models with a single {{GPU}},'' \emph{arXiv preprint}, no. 2303.06865, June 2023. [Online]. Available: \url{https://arxiv.org/abs/2303.06865}
\BIBentrySTDinterwordspacing

\bibitem{alizadehLLMFlashEfficient2024}
\BIBentryALTinterwordspacing
K.~Alizadeh, I.~Mirzadeh, D.~Belenko, K.~Khatamifard, M.~Cho, C.~C. Del~Mundo, M.~Rastegari, and M.~Farajtabar, ``{{LLM}} in a {{Flash}}: Efficient large language model inference with limited memory,'' \emph{arXiv preprint}, no. 2312.11514, Jan. 2024. [Online]. Available: \url{https://arxiv.org/abs/2312.11514}
\BIBentrySTDinterwordspacing

\bibitem{nvidiaRapidsaiKvikioKvikIO2022}
\BIBentryALTinterwordspacing
{Nvidia}, ``{{KvikIO}} - high performance file {{IO}},'' 2022, accessed 07/21/2024. [Online]. Available: \url{https://github.com/rapidsai/kvikio}
\BIBentrySTDinterwordspacing

\bibitem{wikipediaMonkeyPatch2024}
\BIBentryALTinterwordspacing
{Wikipedia}, ``Monkey patch,'' 2006. [Online]. Available: \url{https://en.wikipedia.org/w/index.php?title=Monkey_patch}
\BIBentrySTDinterwordspacing

\bibitem{caiFlashCorrectandrefreshRetentionaware2012}
\BIBentryALTinterwordspacing
Y.~Cai, G.~Yalcin, O.~Mutlu, E.~F. Haratsch, A.~Cristal, O.~S. Unsal, and K.~Mai, ``{{Flash Correct-and-Refresh}}: Retention-aware error management for increased {{Flash}} memory lifetime,'' in \emph{2012 {{IEEE}} 30th {{International Conference}} on {{Computer Design}} ({{ICCD}})}, 2012. [Online]. Available: \url{https://ieeexplore.ieee.org/abstract/document/6378623} pp. 94--101.
\BIBentrySTDinterwordspacing

\bibitem{yucaiErrorPatternsMLC2012}
\BIBentryALTinterwordspacing
Y.~Cai, E.~F. Haratsch, O.~Mutlu, and K.~Mai, ``Error patterns in {{MLC NAND}} {{Flash}} memory: Measurement, characterization, and analysis,'' in \emph{2012 {{Design}}, {{Automation}} \& {{Test}} in {{Europe Conference}} \& {{Exhibition}} ({{DATE}})}.\hskip 1em plus 0.5em minus 0.4em\relax IEEE, 2012. [Online]. Available: \url{http://ieeexplore.ieee.org/document/6176524/} pp. 521--526.
\BIBentrySTDinterwordspacing

\bibitem{liuOptimizingNANDFlashBased2012}
R.-S. Liu, C.-L. Yang, and W.~Wu, ``Optimizing {{NAND}} {{Flash}}-based {{SSDs}} via retention relaxation,'' in \emph{10th {{USENIX Conference}} on {{File}} and {{Storage Technologies}} ({{FAST}} 12)}.\hskip 1em plus 0.5em minus 0.4em\relax USENIX Association, 2012.

\bibitem{kimBehemothFlashcentricTraining2021}
\BIBentryALTinterwordspacing
S.~Kim, Y.~Jin, G.~Sohn, J.~Bae, T.~J. Ham, and J.~W. Lee, ``Behemoth: A {{Flash}}-centric training accelerator for extreme-scale {{DNNs}},'' in \emph{Proceedings of the 19th {{USENIX Conference}} on {{File}} and {{Storage Technologies}}}, 2021. [Online]. Available: \url{https://www.usenix.org/conference/fast21/presentation/kim} pp. 371--385.
\BIBentrySTDinterwordspacing

\bibitem{ortiz-suarez-etal-2020-monolingual}
\BIBentryALTinterwordspacing
P.~J. Ortiz~Su{'a}rez, L.~Romary, and B.~Sagot, ``A monolingual approach to contextualized word embeddings for mid-resource languages,'' in \emph{Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics}.\hskip 1em plus 0.5em minus 0.4em\relax Online: Association for Computational Linguistics, July 2020. [Online]. Available: \url{https://www.aclweb.org/anthology/2020.acl-main.156} pp. 1703--1714.
\BIBentrySTDinterwordspacing

\bibitem{OrtizSuarezSagotRomary2019}
\BIBentryALTinterwordspacing
P.~J.~O. Su{'a}rez, B.~Sagot, and L.~Romary, ``\BIBforeignlanguage{en}{Asynchronous pipelines for processing huge corpora on medium to low resource infrastructures},'' in \emph{\BIBforeignlanguage{en}{Proceedings of the Workshop on Challenges in the Management of Large Corpora (CMLC-7) 2019. Cardiff, 22nd July 2019}}, P.~Bański, A.~Barbaresi, H.~Biber, E.~Breiteneder, S.~Clematide, M.~Kupietz, H.~L{"u}ngen, and C.~Iliadi, Eds.\hskip 1em plus 0.5em minus 0.4em\relax Mannheim: Leibniz-Institut f{"u}r Deutsche Sprache, 2019. [Online]. Available: \url{http://nbn-resolving.de/urn:nbn:de:bsz:mh39-90215} pp. 9 -- 16.
\BIBentrySTDinterwordspacing

\bibitem{daoFlashAttention2FasterAttention2023}
\BIBentryALTinterwordspacing
T.~Dao, ``{{FlashAttention-2}}: Faster attention with better parallelism and work partitioning,'' 2023. [Online]. Available: \url{http://arxiv.org/abs/2307.08691}
\BIBentrySTDinterwordspacing

\bibitem{nitinScalingLargeLanguage2023}
\BIBentryALTinterwordspacing
Nitin and Q.~Zhang, ``Scaling large language model training with {{Pax}} on {{GPUs}},'' 2023. [Online]. Available: \url{https://www.nvidia.com/en-us/on-demand/session/gtcspring23-s51800/}
\BIBentrySTDinterwordspacing

\bibitem{daoFlashAttentionFastMemoryEfficient2022}
\BIBentryALTinterwordspacing
T.~Dao, D.~Y. Fu, S.~Ermon, A.~Rudra, and C.~Ré, ``{{FlashAttention}}: Fast and memory-efficient exact attention with {{IO}}-awareness,'' 2022. [Online]. Available: \url{http://arxiv.org/abs/2205.14135}
\BIBentrySTDinterwordspacing

\bibitem{googlePaxmlAkaPax2022}
\BIBentryALTinterwordspacing
{Google}, ``Paxml (aka {{Pax}}),'' 2022. [Online]. Available: \url{https://github.com/google/paxml}
\BIBentrySTDinterwordspacing

\bibitem{dihuniNVIDIAA1009002100100000002021}
\BIBentryALTinterwordspacing
{Dihuni}, ``{{NVIDIA A100}} 900-21001-0000-000 {{40GB Ampere PCIe GPU}} for deep learning {{AI}}, {{HPC}}, analytics and research – {{Dihuni}} – {{GPU}} server for {{AI}}, data center \& {{IoT}} hardware \& software solutions,'' 2021. [Online]. Available: \url{https://www.dihuni.com/product/nvidia-a100-900-21001-0000-000-40gb-ampere-pcie-gpu-for-deep-learning/}
\BIBentrySTDinterwordspacing

\bibitem{neweggIntelOptaneDC2021}
\BIBentryALTinterwordspacing
{Newegg}, ``Intel {{Optane DC P5800X}} series 1.{{6TB}}, 2.5" x 15mm, {{U}}.2, {{PCIe}} 4.0 x4, {{3D XPoint}} solid state drive ({{SSD}}) ssdpf21q016tb01 - newegg.com,'' 2021. [Online]. Available: \url{https://www.newegg.com/intel-optane-ssd-dc-p5800x-1-6tb/p/N82E16820167481}
\BIBentrySTDinterwordspacing

\bibitem{samsungSamsungVNANDSSD2021}
\BIBentryALTinterwordspacing
{Samsung}, ``Samsung {{V-NAND SSD}} 980 pro 2021 data sheet revision 2.1,'' 2021. [Online]. Available: \url{https://download.semiconductor.samsung.com/resources/data-sheet/Samsung-NVMe-SSD-980-PRO-Data-Sheet_Rev.2.1_230509_10129505081019.pdf}
\BIBentrySTDinterwordspacing

\bibitem{bestbuySamsung980PRO2022}
\BIBentryALTinterwordspacing
{Best Buy}, ``Samsung 980 pro {{1TB}} internal gaming {{SSD}} {{PCIe}} gen 4 x4 nvme mz-v8p1t0b/am,'' 2022. [Online]. Available: \url{https://www.bestbuy.com/site/samsung-980-pro-1tb-internal-gaming-ssd-pcie-gen-4-x4-nvme/6431939.p}
\BIBentrySTDinterwordspacing

\bibitem{stavrinosDonBeBlockhead2021}
\BIBentryALTinterwordspacing
T.~Stavrinos, D.~S. Berger, E.~Katz-Bassett, and W.~Lloyd, ``Don't be a blockhead: Zoned namespaces make work on conventional {{SSDs}} obsolete,'' in \emph{Proceedings of the {{Workshop}} on {{Hot Topics}} in {{Operating Systems}}}.\hskip 1em plus 0.5em minus 0.4em\relax ACM, 2021. [Online]. Available: \url{https://dl.acm.org/doi/10.1145/3458336.3465300} pp. 144--151.
\BIBentrySTDinterwordspacing

\bibitem{hanZNSAdvancedZoned2021}
K.~Han, H.~Gwak, D.~Shin, and J.-Y. Hwang, ``{{ZNS}}+: Advanced zoned namespace interface for supporting in-storage zone compaction,'' in \emph{Proceedings of the 15th {{USENIX Symposium}} on {{Operating Systems Design}} and {{Implementation}}}, 2021, pp. 147--162.

\bibitem{barrosoDatacenterComputerDesigning2019}
\BIBentryALTinterwordspacing
L.~A. Barroso, U.~Hölzle, and P.~Ranganathan, \emph{The Datacenter as a Computer: Designing Warehouse-Scale Machines}, ser. Synthesis Lectures on {{Computer Architecture}}.\hskip 1em plus 0.5em minus 0.4em\relax Springer International Publishing, 2019. [Online]. Available: \url{https://link.springer.com/10.1007/978-3-031-01761-2}
\BIBentrySTDinterwordspacing

\bibitem{redoakconsultingTotalCostOwnership2024}
\BIBentryALTinterwordspacing
{Red Oak Consulting}, ``Total cost of ownership ({{TCO}}) analysis,'' 2024. [Online]. Available: \url{https://www.redoakconsulting.co.uk/tco/}
\BIBentrySTDinterwordspacing

\bibitem{dylanpatelNvidiaBlackwellPerf2024}
\BIBentryALTinterwordspacing
D.~Patel and D.~Nishball, ``Nvidia {{Blackwell}} perf {{TCO}} analysis – {{B100}} vs {{B200}} vs {{GB200NVL72}} – {{SemiAnalysis}},'' 2024. [Online]. Available: \url{https://semianalysis.com/2024/04/10/nvidia-blackwell-perf-tco-analysis/}
\BIBentrySTDinterwordspacing

\bibitem{sniaSNIAEnterpriseTCO2020}
\BIBentryALTinterwordspacing
{SNIA}, ``{{SNIA}} enterprise {{TCO}} calculator,'' 2020. [Online]. Available: \url{https://www.snia.org/sites/default/files/SSSI/SNIA%20TCO%20%20rev1%20generic%2012-2020.xlsx}
\BIBentrySTDinterwordspacing

\bibitem{nvidiaIntroductionNVIDIADGX2023}
\BIBentryALTinterwordspacing
{Nvidia}, ``Introduction to {{NVIDIA DGX H100}}/{{H200}} systems — {{NVIDIA DGX H100}}/{{H200}} user guide,'' 2023. [Online]. Available: \url{https://docs.nvidia.com/dgx/dgxh100-user-guide/introduction-to-dgxh100.html}
\BIBentrySTDinterwordspacing

\bibitem{nvmexpressinc.NVMExpressMoves2016}
\BIBentryALTinterwordspacing
{NVM Express, Inc.}, ``{{NVM Express}} moves into the future,'' 2016. [Online]. Available: \url{https://nvmexpress.org/wp-content/uploads/NVMe_Over_Fabrics.pdf}
\BIBentrySTDinterwordspacing

\bibitem{handwikiSoftwareLustreFile2021}
\BIBentryALTinterwordspacing
{HandWiki}, ``Software:{{Lustre}} (file system),'' 2021. [Online]. Available: \url{https://handwiki.org/wiki/Software:Lustre_(file_system)}
\BIBentrySTDinterwordspacing

\bibitem{raviGPUDirectHDF52020}
\BIBentryALTinterwordspacing
J.~Ravi, S.~Byna, and Q.~Koziol, ``{{GPU Direct I}}/{{O}} with {{HDF5}},'' in \emph{2020 {{IEEE}}/{{ACM Fifth International Parallel Data Systems Workshop}} ({{PDSW}})}, 2020. [Online]. Available: \url{https://www.hdfgroup.org/wp-content/uploads/2020/10/GPU_Direct_IO_with_HDF5-_John_Ravi.pdf} pp. 28--33.
\BIBentrySTDinterwordspacing

\bibitem{kwonEfficientMemoryManagement2023}
\BIBentryALTinterwordspacing
W.~Kwon, Z.~Li, S.~Zhuang, Y.~Sheng, L.~Zheng, C.~H. Yu, J.~Gonzalez, H.~Zhang, and I.~Stoica, ``Efficient memory management for large language model serving with {{PagedAttention}},'' in \emph{Proceedings of the 29th {{Symposium}} on {{Operating Systems Principles}}}.\hskip 1em plus 0.5em minus 0.4em\relax ACM, 2023. [Online]. Available: \url{https://dl.acm.org/doi/10.1145/3600006.3613165} pp. 611--626.
\BIBentrySTDinterwordspacing

\bibitem{pengCapuchinTensorbasedGPU2020}
X.~Peng, X.~Shi, H.~Dai, H.~Jin, W.~Ma, Q.~Xiong, F.~Yang, and X.~Qian, ``Capuchin: Tensor-based {{GPU}} memory management for deep learning,'' in \emph{Proceedings of the {{Twenty-Fifth International Conference}} on {{Architectural Support}} for {{Programming Languages}} and {{Operating Systems}}}.\hskip 1em plus 0.5em minus 0.4em\relax Lausanne Switzerland: ACM, Mar. 2020, pp. 891--905.

\bibitem{wangSuperNeuronsDynamicGPU2018}
L.~Wang, J.~Ye, Y.~Zhao, W.~Wu, A.~Li, S.~L. Song, Z.~Xu, and T.~Kraska, ``{{SuperNeurons}}: Dynamic {{GPU}} memory management for training deep neural networks,'' in \emph{Proceedings of the 23rd {{ACM SIGPLAN Symposium}} on {{Principles}} and {{Practice}} of {{Parallel Programming}}}, Feb. 2018, pp. 41--53.

\bibitem{rhuVDNNVirtualizedDeep2016}
M.~Rhu, N.~Gimelshein, J.~Clemons, A.~Zulfiqar, and S.~W. Keckler, ``{{vDNN}}: Virtualized deep neural networks for scalable, memory-efficient neural network design,'' July 2016.

\bibitem{huangSwapAdvisorPushingDeep2020}
C.-C. Huang, G.~Jin, and J.~Li, ``{{SwapAdvisor}}: Pushing deep learning beyond the {{GPU}} memory limit via smart swapping,'' in \emph{Proceedings of the {{Twenty-Fifth International Conference}} on {{Architectural Support}} for {{Programming Languages}} and {{Operating Systems}}}.\hskip 1em plus 0.5em minus 0.4em\relax Lausanne Switzerland: ACM, Mar. 2020, pp. 1341--1355.

\bibitem{nvidiaNVIDIAH100Tensor2023}
\BIBentryALTinterwordspacing
{Nvidia}, ``{{NVIDIA}} {{H100}} tensor core {{GPU}} architecture,'' 2023. [Online]. Available: \url{https://resources.nvidia.com/en-us-tensor-core}
\BIBentrySTDinterwordspacing

\bibitem{zaheerBigBirdTransformers2020}
M.~Zaheer, G.~Guruganesh, A.~Dubey, J.~Ainslie, C.~Alberti, S.~Ontanon, P.~Pham, A.~Ravula, Q.~Wang, L.~Yang, and A.~Ahmed, ``{{Big Bird}}: Transformers for longer sequences,'' in \emph{Proceedings of the 34th {{International Conference}} on {{Neural Information Processing Systems}}}, ser. {{NIPS}} '20.\hskip 1em plus 0.5em minus 0.4em\relax Curran Associates Inc., 2020, pp. 17\,283--17\,297.

\bibitem{liuDejaVuContextual2023}
Z.~Liu, J.~Wang, T.~Dao, T.~Zhou, B.~Yuan, Z.~Song, A.~Shrivastava, C.~Zhang, Y.~Tian, C.~Ré, and B.~Chen, ``Deja {{Vu}}: Contextual sparsity for efficient {{LLMs}} at inference time,'' in \emph{Proceedings of the 40th {{International Conference}} on {{Machine Learning}}}, ser. {{ICML}} '23.\hskip 1em plus 0.5em minus 0.4em\relax JMLR.org, 2023.

\bibitem{kimSqueezeLLMDenseandSparseQuantization2024}
\BIBentryALTinterwordspacing
S.~Kim, C.~Hooper, A.~Gholami, Z.~Dong, X.~Li, S.~Shen, M.~W. Mahoney, and K.~Keutzer, ``{{SqueezeLLM}}: Dense-and-sparse quantization,'' 2024. [Online]. Available: \url{http://arxiv.org/abs/2306.07629}
\BIBentrySTDinterwordspacing

\bibitem{dettmersSpQRSparseQuantizedRepresentation2023}
\BIBentryALTinterwordspacing
T.~Dettmers, R.~Svirschevski, V.~Egiazarian, D.~Kuznedelev, E.~Frantar, S.~Ashkboos, A.~Borzunov, T.~Hoefler, and D.~Alistarh, ``{{SpQR}}: A sparse-quantized representation for near-lossless {{LLM}} weight compression,'' 2023. [Online]. Available: \url{http://arxiv.org/abs/2306.03078}
\BIBentrySTDinterwordspacing

\bibitem{frantarSparseGPTMassiveLanguage2023}
E.~Frantar and D.~Alistarh, ``{{SparseGPT}}: Massive language models can be accurately pruned in one-shot,'' in \emph{Proceedings of the 40th {{International Conference}} on {{Machine Learning}}}, ser. {{ICML}} '23.\hskip 1em plus 0.5em minus 0.4em\relax JMLR.org, 2023.

\bibitem{shazeerOutrageouslyLargeNeural2017a}
\BIBentryALTinterwordspacing
N.~Shazeer, A.~Mirhoseini, K.~Maziarz, A.~Davis, Q.~Le, G.~Hinton, and J.~Dean, ``Outrageously large neural networks: The sparsely-gated mixture-of-experts layer,'' 2017. [Online]. Available: \url{http://arxiv.org/abs/1701.06538}
\BIBentrySTDinterwordspacing

\bibitem{zhouLearningFinegrainedStructured2021}
\BIBentryALTinterwordspacing
A.~Zhou, Y.~Ma, J.~Zhu, J.~Liu, Z.~Zhang, K.~Yuan, W.~Sun, and H.~Li, ``Learning n:m fine-grained structured sparse neural networks from scratch,'' 2021. [Online]. Available: \url{http://arxiv.org/abs/2102.04010}
\BIBentrySTDinterwordspacing

\bibitem{poolChannelPermutationsSparsity2021}
\BIBentryALTinterwordspacing
J.~Pool and C.~Yu, ``Channel permutations for {{N}}:{{M}} sparsity,'' in \emph{Advances in {{Neural Information Processing Systems}}}, vol.~34.\hskip 1em plus 0.5em minus 0.4em\relax Curran Associates, Inc., 2021. [Online]. Available: \url{https://proceedings.neurips.cc/paper_files/paper/2021/hash/6e8404c3b93a9527c8db241a1846599a-Abstract.html} pp. 13\,316--13\,327.
\BIBentrySTDinterwordspacing

\bibitem{galeMegaBlocksEfficientSparse2023}
\BIBentryALTinterwordspacing
T.~Gale, D.~Narayanan, C.~Young, and M.~Zaharia, ``{{MegaBlocks}}: Efficient sparse training with mixture-of-experts,'' in \emph{Proceedings of {{Machine Learning}} and {{Systems}} 5 ({{MLSys}} 2023)}, vol.~5.\hskip 1em plus 0.5em minus 0.4em\relax Curan, 2023. [Online]. Available: \url{https://proceedings.mlsys.org/paper_files/paper/2023/file/5a54f79333768effe7e8927bcccffe40-Paper-mlsys2023.pdf} pp. 288--304.
\BIBentrySTDinterwordspacing

\bibitem{zhengSparTADeepLearningModel2022}
N.~Zheng, B.~Lin, Q.~Zhang, L.~Ma, Y.~Yang, F.~Yang, Y.~Wang, M.~Yang, and L.~Zhou, ``{{SparTA}}: Deep-learning model sparsity via tensor-with-sparsity-attribute,'' in \emph{Proceedings of the 16th {{USENIX Symposium}} on {{Operating Systems Design}} and {{Implementation}}}, 2022.

\bibitem{galeSparseGPUKernels2020}
\BIBentryALTinterwordspacing
T.~Gale, M.~Zaharia, C.~Young, and E.~Elsen, ``Sparse {{GPU}} kernels for deep learning,'' in \emph{{{SC20}}: {{International Conference}} for {{High Performance Computing}}, {{Networking}}, {{Storage}} and {{Analysis}}}.\hskip 1em plus 0.5em minus 0.4em\relax IEEE, 2020. [Online]. Available: \url{https://ieeexplore.ieee.org/document/9355309/} pp. 1--14.
\BIBentrySTDinterwordspacing

\bibitem{shigangliEfficientQuantizedSparse2024}
\BIBentryALTinterwordspacing
S.~Li, K.~Osawa, and T.~Hoefler, ``Efficient quantized sparse matrix operations on tensor cores,'' in \emph{Proceedings of the {{International Conference}} on {{High Performance Computing}}, {{Networking}}, {{Storage}} and {{Analysis}}}, ser. {{SC}}'22.\hskip 1em plus 0.5em minus 0.4em\relax IEEE Press, 2021. [Online]. Available: \url{https://github.com/Shigangli/Magicube}
\BIBentrySTDinterwordspacing

\bibitem{chenDynamicFineGrainedStructured2023}
\BIBentryALTinterwordspacing
Z.~Chen, Z.~Qu, Y.~Quan, L.~Liu, Y.~Ding, and Y.~Xie, ``Dynamic {{N}}:{{M}} fine-grained structured sparse attention mechanism,'' in \emph{Proceedings of the 28th {{ACM SIGPLAN Annual Symposium}} on {{Principles}} and {{Practice}} of {{Parallel Programming}}}, ser. {{PPoPP}} '23.\hskip 1em plus 0.5em minus 0.4em\relax Association for Computing Machinery, 2023. [Online]. Available: \url{https://dl.acm.org/doi/10.1145/3572848.3577500} pp. 369--379.
\BIBentrySTDinterwordspacing

\bibitem{mishraAcceleratingSparseDeep2021}
\BIBentryALTinterwordspacing
A.~Mishra, J.~A. Latorre, J.~Pool, D.~Stosic, D.~Stosic, G.~Venkatesh, C.~Yu, and P.~Micikevicius, ``Accelerating sparse deep neural networks,'' 2021. [Online]. Available: \url{http://arxiv.org/abs/2104.08378}
\BIBentrySTDinterwordspacing

\bibitem{nvidiaNVIDIATensorRTLLM2023}
\BIBentryALTinterwordspacing
{Nvidia}, ``{{TensorRT-LLM}}: A {{TensorRT}} toolbox for optimized large language model inference,'' 2023. [Online]. Available: \url{https://github.com/NVIDIA/TensorRT-LLM}
\BIBentrySTDinterwordspacing

\bibitem{nvidiaNVIDIATransformerEngine2023}
\BIBentryALTinterwordspacing
{Nvidia}, ``Transformer engine,'' 2023. [Online]. Available: \url{https://github.com/NVIDIA/TransformerEngine}
\BIBentrySTDinterwordspacing

\bibitem{anthonybarbierAddNewKeys2022}
\BIBentryALTinterwordspacing
A.~Barbier, ``Add new keys for {{Graphcore IPU}} ({{DispatchKey}} / {{Backend}} / {{DeviceType}}) by {{AnthonyBarbier}} · pull request \#74763 · pytorch/pytorch,'' 2022. [Online]. Available: \url{https://github.com/pytorch/pytorch/pull/74763}
\BIBentrySTDinterwordspacing

\bibitem{minjiewangReleaseV080Dmlc2022}
\BIBentryALTinterwordspacing
M.~Wang, ``Release v0.8.0 · dmlc/dgl,'' 2022. [Online]. Available: \url{https://github.com/dmlc/dgl/releases/tag/0.8.0}
\BIBentrySTDinterwordspacing

\bibitem{seungwonminDocAddOfficial2021}
\BIBentryALTinterwordspacing
S.~W. Min, ``[doc] add an official documentation of {{UnifiedTensor}} by davidmin7 · pull request \#3194 · dmlc/dgl,'' 2021. [Online]. Available: \url{https://github.com/dmlc/dgl/pull/3194}
\BIBentrySTDinterwordspacing

\bibitem{seungwonminFeatureAddMultiGPU2021}
\BIBentryALTinterwordspacing
S.~W. Min, ``[feature] add multi-{{GPU UnifiedTensor}} unit test by davidmin7 · pull request \#3184 · dmlc/dgl,'' 2021. [Online]. Available: \url{https://github.com/dmlc/dgl/pull/3184}
\BIBentrySTDinterwordspacing

\bibitem{seungwonminFeaturePerformanceGPUIntroducingUnifiedTensor2021}
\BIBentryALTinterwordspacing
S.~W. Min, ``[feature][performance][{{GPU}}] introducing {{UnifiedTensor}} for efficient zero-copy host memory access from {{GPU}} by davidmin7 · pull request \#3086 · dmlc/dgl,'' 2021. [Online]. Available: \url{https://github.com/dmlc/dgl/pull/3086}
\BIBentrySTDinterwordspacing

\bibitem{xinyaoDglDGLGraphpin_memory_2022}
\BIBentryALTinterwordspacing
X.~Yao and J.~Zhou, ``{{dgl}}.{{DGLGraph}}.{{p}}in\_memory\_,'' 2022. [Online]. Available: \url{https://docs.dgl.ai/en/2.0.x/generated/dgl.DGLGraph.pin_memory_.html}
\BIBentrySTDinterwordspacing

\bibitem{zhaoBridgingGapDeep2018}
\BIBentryALTinterwordspacing
Y.~Zhao, J.~Li, C.~Liao, and X.~Shen, ``Bridging the gap between deep learning and sparse matrix format selection,'' in \emph{Proceedings of the 23rd {{ACM SIGPLAN Symposium}} on {{Principles}} and {{Practice}} of {{Parallel Programming}}}.\hskip 1em plus 0.5em minus 0.4em\relax ACM, 2018. [Online]. Available: \url{https://dl.acm.org/doi/10.1145/3178487.3178495} pp. 94--108.
\BIBentrySTDinterwordspacing

\bibitem{almasriParallelizingMaximalClique2023}
\BIBentryALTinterwordspacing
M.~Almasri, Y.-H. Chang, I.~E. Hajj, R.~Nagi, J.~Xiong, and W.-M. Hwu, ``Parallelizing maximal clique enumeration on {{GPUs}},'' in \emph{2023 32nd {{International Conference}} on {{Parallel Architectures}} and {{Compilation Techniques}} ({{PACT}})}, 2023. [Online]. Available: \url{https://ieeexplore.ieee.org/document/10364576/} pp. 162--175.
\BIBentrySTDinterwordspacing

\bibitem{kawtikwarHyLACHybridLinear2024}
\BIBentryALTinterwordspacing
S.~Kawtikwar and R.~Nagi, ``{{HyLAC}}: Hybrid linear assignment solver in {{CUDA}},'' \emph{Journal of Parallel and Distributed Computing}, vol. 187, p. 104838, 2024. [Online]. Available: \url{https://linkinghub.elsevier.com/retrieve/pii/S0743731524000029}
\BIBentrySTDinterwordspacing

\bibitem{nvidiaControllingDataMovement2020}
\BIBentryALTinterwordspacing
Nvidia, ``Controlling data movement to boost performance on the {{NVIDIA}} ampere architecture,'' Sep. 2020. [Online]. Available: \url{https://developer.nvidia.com/blog/controlling-data-movement-to-boost-performance-on-ampere-architecture/}
\BIBentrySTDinterwordspacing

\bibitem{zhouMixtureofExpertsExpertChoice2022}
\BIBentryALTinterwordspacing
Y.~Zhou, T.~Lei, H.~Liu, N.~Du, Y.~Huang, V.~Zhao, A.~Dai, Z.~Chen, Q.~Le, and J.~Laudon, ``Mixture-of-experts with expert choice routing.'' [Online]. Available: \url{http://arxiv.org/abs/2202.09368}
\BIBentrySTDinterwordspacing

\bibitem{kaggle2017KaggleMachine}
\BIBentryALTinterwordspacing
{Kaggle}, ``2017 {{Kaggle}} machine learning \& data science survey,'' 2017. [Online]. Available: \url{https://www.kaggle.com/datasets/kaggle/kaggle-survey-2017}
\BIBentrySTDinterwordspacing

\bibitem{crowdflower2017DataScientist2017}
\BIBentryALTinterwordspacing
{CrowdFlower}, ``2017 data scientist report,'' 2017. [Online]. Available: \url{https://visit.figure-eight.com/rs/416-ZBE-142/images/CrowdFlower_DataScienceReport.pdf}
\BIBentrySTDinterwordspacing

\bibitem{DoingRealityCheck2018}
\BIBentryALTinterwordspacing
J.~Kobielus, ``Doing a reality check on {{GPU}}-accelerated databases,'' 2018. [Online]. Available: \url{https://siliconangle.com/2018/11/09/reality-check-gpu-accelerated-databases/}
\BIBentrySTDinterwordspacing

\bibitem{nvidiaRAPIDSGPUAcceleratedData2018}
\BIBentryALTinterwordspacing
{Nvidia}, ``{{RAPIDS}}: {{GPU}}-accelerated data analytics \& machine learning,'' 2018. [Online]. Available: \url{https://developer.nvidia.com/rapids}
\BIBentrySTDinterwordspacing

\bibitem{caoGPUDatabaseSystems2023}
\BIBentryALTinterwordspacing
J.~Cao, R.~Sen, M.~Interlandi, J.~Arulraj, and H.~Kim, ``{{GPU}} database systems characterization and optimization,'' \emph{Proceedings of the VLDB Endowment}, vol.~17, no.~3, pp. 441--454, 2023. [Online]. Available: \url{https://dl.acm.org/doi/10.14778/3632093.3632107}
\BIBentrySTDinterwordspacing

\bibitem{xiangyaoyuGPUDatabasesNew2024}
\BIBentryALTinterwordspacing
X.~Yu, ``{{GPU}} databases—the new modality of data analytics,'' 2024. [Online]. Available: \url{https://uwaterloo.ca/data-systems-group/sites/ca.data-systems-group/files/uploads/files/talk-xiangyao-3-12.pdf}
\BIBentrySTDinterwordspacing

\bibitem{optimizdbaDatabaseOptimizationTechniques2018}
\BIBentryALTinterwordspacing
{OptimizDBA}, ``Database optimization techniques \#1: Indexing,'' 2018. [Online]. Available: \url{https://optimizdba.com/database-optimization-techniques-1-indexing/}
\BIBentrySTDinterwordspacing

\bibitem{oracleMySQLMySQL842024}
\BIBentryALTinterwordspacing
{Oracle}, ``{{MySQL}} :: {{MySQL}} 8.4 reference manual :: 10.3 optimization and indexes,'' 2024. [Online]. Available: \url{https://dev.mysql.com/doc/refman/8.4/en/optimization-indexes.html}
\BIBentrySTDinterwordspacing

\bibitem{IndexTable2023}
\BIBentryALTinterwordspacing
``Index on table,'' 2023, HEAVY.AI Support Portal. [Online]. Available: \url{http://support.heavy.ai/hc/en-us/community/posts/11659059937047-Index-on-table}
\BIBentrySTDinterwordspacing

\bibitem{sqreamSQreamsUniqueArchitecture2024}
\BIBentryALTinterwordspacing
{SQream}, ``{{SQream}}'s unique architecture: Comparing and contrasting to leading data architectures,'' 2024. [Online]. Available: \url{https://info.sqream.com/hubfs/Ask%20bigger%20resources/SQream%E2%80%99s%20Unique%20Architecture%20Whitepaper.pdf}
\BIBentrySTDinterwordspacing

\bibitem{palkarEvaluatingEndendOptimization2018}
\BIBentryALTinterwordspacing
S.~Palkar, J.~Thomas, D.~Narayanan, P.~Thaker, R.~Palamuttam, P.~Negi, A.~Shanbhag, M.~Schwarzkopf, H.~Pirk, S.~Amarasinghe, S.~Madden, and M.~Zaharia, ``Evaluating end-to-end optimization for data analytics applications in {{Weld}},'' \emph{Proceedings of the VLDB Endowment}, vol.~11, no.~9, pp. 1002--1015, 2018. [Online]. Available: \url{https://dl.acm.org/doi/10.14778/3213880.3213890}
\BIBentrySTDinterwordspacing

\bibitem{nakandalaTensorCompilerUnified2020}
S.~Nakandala, K.~Saur, G.-I. Yu, K.~Karanasos, C.~Curino, M.~Weimer, and M.~Interlandi, ``A tensor compiler for uniﬁed machine learning prediction serving,'' in \emph{Proceedings of the 14th {{USENIX Symposium}} on {{Operating Systems Design}} and {{Implementation}}}, 2020.

\end{thebibliography}
