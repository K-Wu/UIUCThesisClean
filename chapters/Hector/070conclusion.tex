\section{Conclusion}
RGNNs are graph neural networks with dedicated structures for modeling the different types of nodes and edges in heterogeneous graphs. While RGNNs have been increasingly adopted in many real-world applications due to their versatility and accuracy, they pose performance and system design challenges: inherent memory-intensive computation patterns, the gap between the programming interface and kernel APIs, and heavy programming effort required to optimize kernels caused by their coupling with data layout and heterogeneity. To systematically address these challenges, we propose Hector, a novel two-level intermediate representation and its code generator framework that 
(a)~\textit{captures} the key properties of RGNN models, and opportunities to reduce memory accesses in inter-operator scheduling and materialization,
(b)~\textit{generates} code with flexible data access schemes to eliminate redundant data copies, and
(c)~\textit{decouples} model semantics, data layout, and operators-specific optimizations from each other to reduce programming effort. 
By building on one GEMM template and a node/edge traversal template, Hector achieves up to 9.9$\times$ speed-up in inference and 43.7$\times$ speed-up in training compared with the state-of-the-art public systems on select models, RGCN, RGAT, and HGT, when running heterogeneous graphs provided by DGL and OGB. 
In addition, Hector does not trigger any OOM exceptions in these tests. 
We also propose linear operator reordering and compact materialization to further accelerate the system by up to 3.8$\times$. 
As an indicator of the reduction of programming effort, Hector takes in 51 lines of code expressing the three models and generates 8K lines of CUDA and C++ code.
Through profiling, we found that higher memory efficiency allows Hector to accommodate larger input and, therefore, attain higher throughput in forward propagation. In contrast, backward propagation is bound by latency introduced by atomic updates and outer products.


