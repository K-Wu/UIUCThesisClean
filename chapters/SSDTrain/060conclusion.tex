\section{Conclusion}

The growth rate of the GPU memory capacity has not been able to keep up with that of the size of LLMs,
hindering the model training process. In particular, activations---the intermediate tensors produced during forward propagation and reused in backward propagation---dominate the GPU memory use. To address this challenge, we propose SSDTrain to efficiently offload activations to high-capacity NVMe SSDs. This approach reduces GPU memory usage without impacting performance by adaptively overlapping data transfers with computation. SSDTrain is compatible with popular deep learning frameworks such as PyTorch, Megatron, and DeepSpeed and employs techniques such as tensor deduplication, forwarding, and adaptive offloading to further enhance efficiency. We extensively tested popular LLMs such as GPT, BERT, and T5. The results demonstrate that SSDTrain effectively reduces 47\% of the activation peak memory usage. At the same time, SSDTrain perfectly overlaps the I/O with the computation and incurs negligible performance overhead. We introduce the ROK curve to compare the SSDTrain offloading with two other tensor placement strategies, keeping activations in GPU memory and layerwise full recomputation.  SSDTrain achieves better memory savings than layerwise full recomputation while retaining the performance of keeping the activations in memory. \kwc{We further analyze how SSDTrain increases training throughput by increasing micro-batch size and reducing pipeline bubbles.}

