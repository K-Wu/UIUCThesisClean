\section{Related Work}\label{sec:ssdtrain_related}
\noindent
\textbf{Swapping and offloading.} Many LLM systems with offloading abilities are inference-only~\cite{kwonEfficientMemoryManagement2023,shengFlexGenHighThroughputGenerative2023,alizadehLLMFlashEfficient2024}. In inference, weights and KV-cache never change and are reused across iterations; researchers leverage this to enhance locality and memory efficiency. However, in LLM training, the weights are updated in each iteration, and all tensors change across the iterations. Some work avails offloading features~\cite{rajbhandariZeROinfinityBreakingGPU2021} for training but is mostly designed to accommodate larger models in a smaller system at the cost of performance. They lack the asynchronous data transfer ability to maintain performance.

Another direction is to offload data and the associated computation to the CPU~\cite{renZeROOffloadDemocratizingBillionScale2021,kamahoriFiddlerCPUGPUOrchestration2024,songPowerInferFastLarge2023}. The offloaded computation is relatively light, and the offloaded data include gradients, sparse elements in the weights, etc. 
Recognizing this direction, SSDTrain is made orthogonal because we offload the activations to SSDs via GDS to minimize the interference with the CPU. Activations are for gradient computation, which is compute-intensive and best done solely on GPUs. 

Before the massive adoption of LLMs, there is work on offloading data for deep learning~\cite{pengCapuchinTensorbasedGPU2020,wangSuperNeuronsDynamicGPU2018,baeFlashNeuronSSDEnabledLargeBatch2021,rhuVDNNVirtualizedDeep2016,huangSwapAdvisorPushingDeep2020}. Most of them offload data to main memory while some~\cite{baeFlashNeuronSSDEnabledLargeBatch2021} enable the GPU--SSD data path. LLM training is unique because massive parallelism and its implications on the memory use of optimizer states, gradients, and weights are fundamental to the design space. SSDTrain naturally supports multiple GPUs. Besides, we demonstrated its viability on clusters and introduced the ROK curve to help with the design choice. On the other hand, LLMs have such a high demand for computing power that it stimulates rapid development in specialized hardware, e.g., transformer engine~\cite{nvidiaNVIDIAH100Tensor2023}, and distributed frameworks. This is why we ensure good interoperability. In contrast, most earlier work in this direction is bound to a specific PyTorch version or a custom runtime with support to select layers.

 
\noindent
\textbf{Quantization and sparsity.} Some work on offloading uses quantization and/or sparsity to reduce the I/O size~\cite{shengFlexGenHighThroughputGenerative2023,alizadehLLMFlashEfficient2024,baeFlashNeuronSSDEnabledLargeBatch2021}.  To reduce computation, algorithms have been proposed to quantize parameters and introduce sparsity into the model~\cite{zaheerBigBirdTransformers2020,liuDejaVuContextual2023,kimSqueezeLLMDenseandSparseQuantization2024,dettmersSpQRSparseQuantizedRepresentation2023,frantarSparseGPTMassiveLanguage2023}. Mixture-of-Experts~(MoE)~\cite{shazeerOutrageouslyLargeNeural2017a} is in this direction as it sparsifies the token-to-neuron connection in the MLP to the token-to-expert connection. Some algorithms introduce structured sparsity, e.g., N:M~\cite{zhouLearningFinegrainedStructured2021} sparsity and 2:4~\cite{poolChannelPermutationsSparsity2021} sparsity. On the other hand, there are frameworks and specialized kernels to accelerate models with quantization and/or sparsity~\cite{galeMegaBlocksEfficientSparse2023,zhengSparTADeepLearningModel2022,galeSparseGPUKernels2020,shigangliEfficientQuantizedSparse2024}. Some kernels leverage specialized hardware, e.g., Ampere tensor core~\cite{chenDynamicFineGrainedStructured2023,mishraAcceleratingSparseDeep2021}.
These techniques are orthogonal to SSDTrain and can be used to alternate the model and accelerate the computation while using SSDTrain. Notably, given the hardware, the reuse factor to fully overlap the computation with PCIe transfer will change according to the new numerical format or sparsity access pattern. We believe that SSDTrain's adaptive offloading algorithm helps optimize the offload amounts in these cases.



\noindent
\textbf{Optimized kernels.} Previous work develops optimized kernels to accelerate LLM~\cite{daoFlashAttentionFastMemoryEfficient2022,daoFlashAttention2FasterAttention2023,nvidiaNVIDIATensorRTLLM2023}. Some kernels utilize special hardware~\cite{nvidiaNVIDIATransformerEngine2023}. SSDTrain's interoperability ensures it can be used easily with these and upcoming techniques.  
